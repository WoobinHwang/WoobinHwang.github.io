<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>WB&#039;blog</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="WB&#039;blog"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="WB&#039;blog"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta property="og:type" content="blog"><meta property="og:title" content="WB&#039;blog"><meta property="og:url" content="https://woobinhwang.github.io/"><meta property="og:site_name" content="WB&#039;blog"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://woobinhwang.github.io/img/og_image.png"><meta property="article:author" content="John Doe"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://woobinhwang.github.io"},"headline":"WB'blog","image":["https://woobinhwang.github.io/img/og_image.png"],"author":{"@type":"Person","name":"John Doe"},"publisher":{"@type":"Organization","name":"WB'blog","logo":{"@type":"ImageObject","url":"https://woobinhwang.github.io/img/logo.svg"}},"description":""}</script><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.svg" alt="WB&#039;blog" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2022-04-17T00:00:00.000Z" title="2022. 4. 17. 오전 9:00:00">2022-04-17</time></span><span class="level-item">Updated&nbsp;<time dateTime="2022-04-17T11:13:09.712Z" title="2022. 4. 17. 오후 8:13:09">2022-04-17</time></span><span class="level-item"> woobin </span><span class="level-item">2 minutes read (About 315 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/04/17/0417_Library_venv_pip3/">라이브러리 / 가상 환경 / pip, pip3 기능</a></h1><div class="content"><h1 id="라이브러리-x2F-가상-환경-x2F-pip-pip3-기능"><a href="#라이브러리-x2F-가상-환경-x2F-pip-pip3-기능" class="headerlink" title="라이브러리 &#x2F; 가상 환경 &#x2F; pip, pip3 기능"></a>라이브러리 &#x2F; 가상 환경 &#x2F; pip, pip3 기능</h1><h3 id="라이브러리-Library"><a href="#라이브러리-Library" class="headerlink" title="라이브러리 (Library):"></a>라이브러리 (Library):</h3><ul>
<li>소프트웨어 개발 시 자주 쓰거나 특정한 기능을 모듈로 만들어 사용자들이 사용 할 수 있게 만들어둔것.</li>
</ul>
<h3 id="가상-환경-virtual-environment"><a href="#가상-환경-virtual-environment" class="headerlink" title="가상 환경 (virtual environment):"></a>가상 환경 (virtual environment):</h3><ul>
<li>작업하는 프로젝트마다 같은 라이브러리를 사용하더라도 쓸 때 마다 버전이 달라지곤 하는데, 이를 해결하기 위해 가상 환경이라는 독립적인 공간을 만들어 해결 할 수 있다.</li>
<li>WSL(Windows Subsystem for Linux)에서 가상 환경을 만들고 적용 (예시)</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ sudo pip3 install virtualenv <span class="comment"># 패키지가 없다면 설치</span></span><br><span class="line">$ virtualenv venv</span><br><span class="line">$ <span class="built_in">source</span> venv/bin/activate</span><br></pre></td></tr></table></figure>

<h3 id="pip-pip3"><a href="#pip-pip3" class="headerlink" title="pip, pip3:"></a>pip, pip3:</h3><ul>
<li><p>pip: python2 버전의 패키지를 설치하는 기능<br>pip3: python3 버전의 패키지를 설치하는 기능<br>—&gt; 파이썬 버전에 맞게 사용</p>
</li>
<li><p>WSL에서 python3 패키지를 패키지를 설치 (예시)</p>
</li>
</ul>
<p>$ pip3 install elasticsearch</p>
<ul>
<li>버전을 확인하는 temp.py를 작성</li>
</ul>
<p><img src="/Images/0417_Library_venv_pip3/Untitled.png" alt="Untitled"></p>
<ul>
<li>버전 확인</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ python3 temp.py</span><br></pre></td></tr></table></figure>

<p><img src="/Images/0417_Library_venv_pip3/Untitled%201.png" alt="Untitled"></p>
<ul>
<li>WSL에서 다운그레이드 하고 버전 확인</li>
</ul>
<p>$ pip3 install elasticsearch&#x3D;&#x3D;7.6.0</p>
<p>$ python3 temp.py</p>
<p><img src="/Images/0417_Library_venv_pip3/Untitled%202.png" alt="Untitled"></p>
<ul>
<li>설치한 패키지 목록</li>
<li>$ pip3 list</li>
</ul>
<p><img src="/Images/0417_Library_venv_pip3/Untitled%203.png" alt="Untitled"></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2022-04-17T00:00:00.000Z" title="2022. 4. 17. 오전 9:00:00">2022-04-17</time></span><span class="level-item">Updated&nbsp;<time dateTime="2022-04-17T11:17:20.683Z" title="2022. 4. 17. 오후 8:17:20">2022-04-17</time></span><span class="level-item"> woobin </span><span class="level-item">a minute read (About 153 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/04/17/0417_VSCode_Remote_function/">VSCode Remote 기능 연동하기</a></h1><div class="content"><h1 id="0417-VSCode-Remote-function"><a href="#0417-VSCode-Remote-function" class="headerlink" title="0417_VSCode_Remote_function"></a>0417_VSCode_Remote_function</h1><p>VSCode에서 remote를 검색하여 설치</p>
<p><img src="/Images/0417_VSCode_Remote_function/Remote.jpg" alt="Untitled"></p>
<ul>
<li>터미널을 켜고 WSL이 있는지 확인</li>
</ul>
<p><img src="/Images/0417_VSCode_Remote_function/Untitled.png" alt="Untitled"></p>
<ul>
<li>WSL에서 가상환경으로 설정</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">source</span> venv/bin/activate</span><br></pre></td></tr></table></figure>

<p><img src="/Images/0417_VSCode_Remote_function/Untitled%201.png" alt="Untitled"></p>
<ul>
<li>Chapter03 파일 속 <a target="_blank" rel="noopener" href="http://hello.py/">hello.py</a> 파일을 만들고 저장</li>
</ul>
<p><img src="/Images/0417_VSCode_Remote_function/Untitled%202.png" alt="Untitled"></p>
<ul>
<li>vscode의 wsl터미널에서 <a target="_blank" rel="noopener" href="http://hello.py/">hello.py</a> 파이썬 파일 실행</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ python3 hello.py</span><br></pre></td></tr></table></figure>

<p><img src="/Images/0417_VSCode_Remote_function/Untitled%203.png" alt="Untitled"></p>
<ul>
<li><a target="_blank" rel="noopener" href="http://hello.py/">hello.py</a> 를 수정하고 다시 실행</li>
</ul>
<p><img src="/Images/0417_VSCode_Remote_function/Untitled%204.png" alt="Untitled"></p>
<p><img src="/Images/0417_VSCode_Remote_function/Untitled%205.png" alt="Untitled"></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2022-04-17T00:00:00.000Z" title="2022. 4. 17. 오전 9:00:00">2022-04-17</time></span><span class="level-item">Updated&nbsp;<time dateTime="2022-04-17T11:36:29.153Z" title="2022. 4. 17. 오후 8:36:29">2022-04-17</time></span><span class="level-item"> woobin </span><span class="level-item">2 minutes read (About 254 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/04/17/0417_PostgreSQL(pgAdmin)_insert/">PostgreSQL 설치</a></h1><div class="content"><h1 id="0417-PostgreSQL-pgAdmin-insert"><a href="#0417-PostgreSQL-pgAdmin-insert" class="headerlink" title="0417_PostgreSQL(pgAdmin)_insert"></a>0417_PostgreSQL(pgAdmin)_insert</h1><ul>
<li><p>설명: 확장 가능성 및 표준 준수를 강조하는 객체-관계형 데이터베이스 관리 시스템의 하나</p>
</li>
<li><p>PostgreSQL 서비스를 실행, 상태 확인</p>
</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ sudo service postgresql strat</span><br><span class="line">$ sudo service postgresql status</span><br></pre></td></tr></table></figure>

<p><img src="/Images/0417_PostgreSQL(pgAdmin)_insert/Untitled.png" alt="Untitled"></p>
<ul>
<li>PostgreSQL 아이디 비밀번호 설정</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo -u postgres psql -c “ALTER USER postgres PASSWORD ‘postgres’;”</span><br></pre></td></tr></table></figure>

<ul>
<li>General탭에서 ‘Name’에 users를 입력(필자는 users2로 작성)</li>
</ul>
<p><img src="/Images/0417_PostgreSQL(pgAdmin)_insert/Untitled%201.png" alt="Untitled"></p>
<ul>
<li>Column탭에는 아래의 테이블을 추가 (Save)</li>
</ul>
<p><img src="/Images/0417_PostgreSQL(pgAdmin)_insert/Untitled%202.png" alt="Untitled"></p>
<ul>
<li>wsl에서 제대로 저장이 되었는지 확인하러 갑니다.<br>(l: List 로 추측됨)</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ sudo -u postgres psql</span><br><span class="line"></span><br><span class="line">$ \l</span><br></pre></td></tr></table></figure>

<p><img src="/Images/0417_PostgreSQL(pgAdmin)_insert/Untitled%203.png" alt="Untitled"></p>
<ul>
<li>dataengineering으로 접속<br>(c: connect 로 추측됨)</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ \c dataengineering</span><br></pre></td></tr></table></figure>

<p><img src="/Images/0417_PostgreSQL(pgAdmin)_insert/Untitled%204.png" alt="Untitled"></p>
<ul>
<li>데이터 테이블을 확인<br>(dt: datatable 로 추측됨)</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ \dt</span><br></pre></td></tr></table></figure>

<p><img src="/Images/0417_PostgreSQL(pgAdmin)_insert/Untitled%205.png" alt="Untitled"></p>
<ul>
<li>제대로 확인이 된다면 빠져나옴.<br>(q: quit 로 추측됨)</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ \q</span><br><span class="line"></span><br><span class="line">$ sudo service postgresql stop</span><br></pre></td></tr></table></figure>

<p><img src="/Images/0417_PostgreSQL(pgAdmin)_insert/Untitled%206.png" alt="Untitled"></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2022-04-17T00:00:00.000Z" title="2022. 4. 17. 오전 9:00:00">2022-04-17</time></span><span class="level-item">Updated&nbsp;<time dateTime="2022-04-18T07:17:59.623Z" title="2022. 4. 18. 오후 4:17:59">2022-04-18</time></span><span class="level-item"> woobin </span><span class="level-item">2 minutes read (About 303 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/04/17/0417_Apache_Airflow_install/">Apache Airflow 설치</a></h1><div class="content"><h1 id="0417-Apache-Airflow-install"><a href="#0417-Apache-Airflow-install" class="headerlink" title="0417_Apache_Airflow_install"></a>0417_Apache_Airflow_install</h1><ul>
<li>airflow를 설치하기 위해 pip를 설치</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt install python3-pip</span><br></pre></td></tr></table></figure>

<ul>
<li>라이브러리 설치</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo pip3 install virtualenv</span><br></pre></td></tr></table></figure>

<ul>
<li>가상환경 생성</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ virtualenv venv</span><br></pre></td></tr></table></figure>

<ul>
<li>가상환경 접속</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$  <span class="built_in">source</span> venv/bin/activate</span><br></pre></td></tr></table></figure>

<ul>
<li>bashrc 설정 (아래 두줄이 같은 결과가 나오게 해야함)</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ vi ~/.bashrc</span><br><span class="line">$ <span class="built_in">source</span> ~/.bashrc</span><br><span class="line"><span class="comment"># $ source venv/bin/activate</span></span><br><span class="line">$ <span class="built_in">echo</span> <span class="variable">$AIRFLOW_HOME</span></span><br><span class="line">$ <span class="built_in">pwd</span></span><br></pre></td></tr></table></figure>

<p><img src="/Images/0417_Apache_Airflow_install/Untitled.png" alt="Untitled"></p>
<ul>
<li>가상환경 취소 &#x2F; 적용하는 방법</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ deactive</span><br><span class="line">$ <span class="built_in">source</span> venv/bin/activate</span><br></pre></td></tr></table></figure>

<p><img src="/Images/0417_Apache_Airflow_install/Untitled%201.png" alt="Untitled"></p>
<ul>
<li>PostgreSQL, Slack, Celery 패키지 설치 (진행중인 상황)</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ pip3 install ‘apache-airflow[postgres, slack, celery]’</span><br></pre></td></tr></table></figure>

<p><img src="/Images/0417_Apache_Airflow_install/Untitled%202.png" alt="Untitled"></p>
<ul>
<li>DB 초기화</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ airflow db init</span><br></pre></td></tr></table></figure>

<p><img src="/Images/0417_Apache_Airflow_install/Untitled%203.png" alt="Untitled"></p>
<p><img src="/Images/0417_Apache_Airflow_install/Untitled%204.png" alt="Untitled"></p>
<ul>
<li>username 설정</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ airflow <span class="built_in">users</span> create --username airflow --password airflow --firstname name --lastname airflow --role Admin --email my@email.com</span><br></pre></td></tr></table></figure>

<p><img src="/Images/0417_Apache_Airflow_install/Untitled%205.png" alt="Untitled"></p>
<ul>
<li>webserver를 실행</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ airflow webserver -p 8081</span><br></pre></td></tr></table></figure>

<ul>
<li>정상적으로 작동 되는지  <a target="_blank" rel="noopener" href="http://localhost:8081/login/">http://localhost:8081/login/</a> 에 접속</li>
</ul>
<p><img src="/Images/0417_Apache_Airflow_install/Untitled%206.png" alt="Untitled"></p>
<ul>
<li>airflow.cfg 파일에 들어가서 load_examples 부분이 True이면 False로 바꾸고 저장.</li>
</ul>
<p><img src="/Images/0417_Apache_Airflow_install/Untitled%207.png" alt="Untitled"></p>
<ul>
<li>DB 초기화</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ airflow db reset</span><br></pre></td></tr></table></figure>

<p><img src="/Images/0417_Apache_Airflow_install/Untitled%208.png" alt="Untitled"></p>
<ul>
<li>웹서버 실행 (DB가 비어있는것을 확인)</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ airflow webserver</span><br></pre></td></tr></table></figure>

<p><img src="/Images/0417_Apache_Airflow_install/Untitled%209.png" alt="Untitled"></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2022-04-13T00:00:00.000Z" title="2022. 4. 13. 오전 9:00:00">2022-04-13</time></span><span class="level-item">Updated&nbsp;<time dateTime="2022-04-13T10:54:02.680Z" title="2022. 4. 13. 오후 7:54:02">2022-04-13</time></span><span class="level-item"> woobin </span><span class="level-item">a few seconds read (About 111 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/04/13/0413_Kibana_Install/">Kibana 설치</a></h1><div class="content"><h1 id="kibana-설치"><a href="#kibana-설치" class="headerlink" title="kibana 설치"></a>kibana 설치</h1><ul>
<li>설명: Elasticsearch 데이터를 시각화하고 Elastic Stack을 탐색하게 해주는 무료 오픈소스 인터페이스</li>
<li>kibana 설치</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt-get install kibana</span><br></pre></td></tr></table></figure>

<p><img src="/Images/0413_Kibana_Install/Untitled.png" alt="Untitled"></p>
<ul>
<li>kibana 서비스를 활성화</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo systemctl <span class="built_in">enable</span> kibana</span><br></pre></td></tr></table></figure>

<p><img src="/Images/0413_Kibana_Install/Untitled%201.png" alt="Untitled"></p>
<ul>
<li>서비스를 시작하고, 확인</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ sudo systemctl start kibana</span><br><span class="line">$ sudo systemctl status kibana</span><br></pre></td></tr></table></figure>

<p><img src="/Images/0413_Kibana_Install/Untitled%202.png" alt="Untitled"></p>
<ul>
<li>로컬에서 확인</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="http://localhost:5601/">http://localhost:5601/</a></strong></p>
<p><img src="/Images/0413_Kibana_Install/Untitled%203.png" alt="Untitled"></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2022-04-12T00:00:00.000Z" title="2022. 4. 12. 오전 9:00:00">2022-04-12</time></span><span class="level-item">Updated&nbsp;<time dateTime="2022-04-13T10:51:03.717Z" title="2022. 4. 13. 오후 7:51:03">2022-04-13</time></span><span class="level-item"> woobin </span><span class="level-item">a minute read (About 194 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/04/12/0412_Multiples_of_3_and_5/">코딩도장 문제 01</a></h1><div class="content"><h1 id="3과-5의-배수의-문제"><a href="#3과-5의-배수의-문제" class="headerlink" title="3과 5의 배수의 문제"></a>3과 5의 배수의 문제</h1><ul>
<li><p>코딩도장에 있는 퀴즈 (추천 1위)</p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://codingdojang.com/scode/350?answer_mode=hide">https://codingdojang.com/scode/350?answer_mode=hide</a></p>
</li>
<li><p>10미만의 자연수에서 3과 5의 배수를 구하면 3,5,6,9이다. 이들의 총합은 23이다.</p>
</li>
<li><p>1000미만의 자연수에서 3,5의 배수의 총합을 구하라.</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 나머지를 구하는 방법: %</span></span><br><span class="line"><span class="comment"># ex)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;%로 나머지를 구하는 방법 : 17 % 4 =&quot;</span>,<span class="number">17</span> % <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 함수를 이용하는 방법: divmod()</span></span><br><span class="line"><span class="comment"># ex)</span></span><br><span class="line"><span class="built_in">divmod</span>(<span class="number">17</span>, <span class="number">4</span>)[<span class="number">1</span>]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;divmod(17, 4):&quot;</span>, <span class="built_in">divmod</span>(<span class="number">17</span>, <span class="number">4</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;divmod(17, 4)[1]:&quot;</span>, <span class="built_in">divmod</span>(<span class="number">17</span>, <span class="number">4</span>)[<span class="number">1</span>])</span><br></pre></td></tr></table></figure>

<pre><code>%로 나머지를 구하는 방법 : 17 % 4 = 1
divmod(17, 4): (4, 1)
divmod(17, 4)[1]: 1
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">quest1 = <span class="number">1000</span></span><br><span class="line">Q_list = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, quest1):</span><br><span class="line">  <span class="keyword">if</span> <span class="built_in">divmod</span>(i, <span class="number">3</span>)[<span class="number">1</span>] == <span class="number">0</span>:</span><br><span class="line">    Q_list.append(i)</span><br><span class="line">  <span class="keyword">elif</span> <span class="built_in">divmod</span>(i, <span class="number">5</span>)[<span class="number">1</span>] == <span class="number">0</span>:</span><br><span class="line">    Q_list.append(i)</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(Q_list)</span><br></pre></td></tr></table></figure>

<pre><code>[3, 5, 6, 9, 10, 12, 15, 18, 20, 21, 24, 25, 27, 30, 33, 35, 36, 39, 40, 42, 45, 48, 50, 51, 54, 55, 57, 60, 63, 65, 66, 69, 70, 72, 75, 78, 80, 81, 84, 85, 87, 90, 93, 95, 96, 99, 100, 102, 105, 108, 110, 111, 114, 115, 117, 120, 123, 125, 126, 129, 130, 132, 135, 138, 140, 141, 144, 145, 147, 150, 153, 155, 156, 159, 160, 162, 165, 168, 170, 171, 174, 175, 177, 180, 183, 185, 186, 189, 190, 192, 195, 198, 200, 201, 204, 205, 207, 210, 213, 215, 216, 219, 220, 222, 225, 228, 230, 231, 234, 235, 237, 240, 243, 245, 246, 249, 250, 252, 255, 258, 260, 261, 264, 265, 267, 270, 273, 275, 276, 279, 280, 282, 285, 288, 290, 291, 294, 295, 297, 300, 303, 305, 306, 309, 310, 312, 315, 318, 320, 321, 324, 325, 327, 330, 333, 335, 336, 339, 340, 342, 345, 348, 350, 351, 354, 355, 357, 360, 363, 365, 366, 369, 370, 372, 375, 378, 380, 381, 384, 385, 387, 390, 393, 395, 396, 399, 400, 402, 405, 408, 410, 411, 414, 415, 417, 420, 423, 425, 426, 429, 430, 432, 435, 438, 440, 441, 444, 445, 447, 450, 453, 455, 456, 459, 460, 462, 465, 468, 470, 471, 474, 475, 477, 480, 483, 485, 486, 489, 490, 492, 495, 498, 500, 501, 504, 505, 507, 510, 513, 515, 516, 519, 520, 522, 525, 528, 530, 531, 534, 535, 537, 540, 543, 545, 546, 549, 550, 552, 555, 558, 560, 561, 564, 565, 567, 570, 573, 575, 576, 579, 580, 582, 585, 588, 590, 591, 594, 595, 597, 600, 603, 605, 606, 609, 610, 612, 615, 618, 620, 621, 624, 625, 627, 630, 633, 635, 636, 639, 640, 642, 645, 648, 650, 651, 654, 655, 657, 660, 663, 665, 666, 669, 670, 672, 675, 678, 680, 681, 684, 685, 687, 690, 693, 695, 696, 699, 700, 702, 705, 708, 710, 711, 714, 715, 717, 720, 723, 725, 726, 729, 730, 732, 735, 738, 740, 741, 744, 745, 747, 750, 753, 755, 756, 759, 760, 762, 765, 768, 770, 771, 774, 775, 777, 780, 783, 785, 786, 789, 790, 792, 795, 798, 800, 801, 804, 805, 807, 810, 813, 815, 816, 819, 820, 822, 825, 828, 830, 831, 834, 835, 837, 840, 843, 845, 846, 849, 850, 852, 855, 858, 860, 861, 864, 865, 867, 870, 873, 875, 876, 879, 880, 882, 885, 888, 890, 891, 894, 895, 897, 900, 903, 905, 906, 909, 910, 912, 915, 918, 920, 921, 924, 925, 927, 930, 933, 935, 936, 939, 940, 942, 945, 948, 950, 951, 954, 955, 957, 960, 963, 965, 966, 969, 970, 972, 975, 978, 980, 981, 984, 985, 987, 990, 993, 995, 996, 999]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">target = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(Q_list)):</span><br><span class="line">  target = target + Q_list[i]</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(target)</span><br></pre></td></tr></table></figure>

<pre><code>233168
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 모범답안:</span></span><br><span class="line"><span class="built_in">sum</span>([x <span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1000</span>) <span class="keyword">if</span> x%<span class="number">3</span>==<span class="number">0</span> <span class="keyword">or</span> x%<span class="number">5</span>==<span class="number">0</span>])</span><br><span class="line"><span class="comment"># 리스트 컴프리헨션 적용한 코드</span></span><br></pre></td></tr></table></figure>




<pre><code>233168
</code></pre>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2022-04-08T00:00:00.000Z" title="2022. 4. 8. 오전 9:00:00">2022-04-08</time></span><span class="level-item">Updated&nbsp;<time dateTime="2022-04-08T04:54:27.371Z" title="2022. 4. 8. 오후 1:54:27">2022-04-08</time></span><span class="level-item"> woobin </span><span class="level-item">5 minutes read (About 762 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/04/08/0408_LSTM,GRU_Cell/">LSTM과 GRU셀</a></h1><div class="content"><h1 id="LSTM과-GPU셀"><a href="#LSTM과-GPU셀" class="headerlink" title="LSTM과 GPU셀"></a>LSTM과 GPU셀</h1><h1 id="LSTM-신경망-훈련하기"><a href="#LSTM-신경망-훈련하기" class="headerlink" title="LSTM 신경망 훈련하기"></a>LSTM 신경망 훈련하기</h1><ul>
<li>RNN은 실무에서 안씀.</li>
<li>LSTM이 나온 배경<ul>
<li>RNN -&gt; 문장이 길면, 학습 능력이 떨어지게됨</li>
<li>Long Short-Term Memory</li>
</ul>
</li>
<li>단기 기억을 오래 기억하기 위해 고안됨.<ul>
<li>메모장에 적듯이 업무 처리함.</li>
</ul>
</li>
</ul>
<h1 id="데이터-불러오기"><a href="#데이터-불러오기" class="headerlink" title="데이터 불러오기"></a>데이터 불러오기</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras.datasets <span class="keyword">import</span> imdb</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line">(train_input, train_target), (test_input, test_target) = imdb.load_data(</span><br><span class="line">    num_words=<span class="number">500</span>)</span><br><span class="line"></span><br><span class="line">train_input, val_input, train_target, val_target = train_test_split(</span><br><span class="line">    train_input, train_target, test_size=<span class="number">0.2</span>, random_state=<span class="number">42</span>)</span><br></pre></td></tr></table></figure>

<pre><code>Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz
17465344/17464789 [==============================] - 0s 0us/step
17473536/17464789 [==============================] - 0s 0us/step
</code></pre>
<h2 id="padding"><a href="#padding" class="headerlink" title="padding"></a>padding</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras.preprocessing.sequence <span class="keyword">import</span> pad_sequences</span><br><span class="line"></span><br><span class="line">train_seq = pad_sequences(train_input, maxlen=<span class="number">100</span>)</span><br><span class="line">val_seq = pad_sequences(val_input, maxlen=<span class="number">100</span>)</span><br></pre></td></tr></table></figure>

<h2 id="모형-만들기"><a href="#모형-만들기" class="headerlink" title="모형 만들기"></a>모형 만들기</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br><span class="line">model = keras.Sequential()</span><br><span class="line">model.add(keras.layers.Embedding(<span class="number">500</span>, <span class="number">16</span>, input_length= <span class="number">100</span>))</span><br><span class="line">model.add(keras.layers.LSTM(<span class="number">8</span>))</span><br><span class="line">model.add(keras.layers.Dense(<span class="number">1</span>, activation= <span class="string">&#x27;sigmoid&#x27;</span>))</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.summary()</span><br></pre></td></tr></table></figure>

<pre><code>Model: &quot;sequential&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 embedding (Embedding)       (None, 100, 16)           8000      
                                                                 
 lstm (LSTM)                 (None, 8)                 800       
                                                                 
 dense (Dense)               (None, 1)                 9         
                                                                 
=================================================================
Total params: 8,809
Trainable params: 8,809
Non-trainable params: 0
_________________________________________________________________
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">rmsprop = keras.optimizers.RMSprop(learning_rate=<span class="number">1e-4</span>)</span><br><span class="line">model.<span class="built_in">compile</span>(optimizer=rmsprop, loss=<span class="string">&#x27;binary_crossentropy&#x27;</span>, </span><br><span class="line">              metrics=[<span class="string">&#x27;accuracy&#x27;</span>])</span><br><span class="line"></span><br><span class="line">checkpoint_cb = keras.callbacks.ModelCheckpoint(<span class="string">&#x27;best-lstm-model.h5&#x27;</span>, </span><br><span class="line">                                                save_best_only=<span class="literal">True</span>)</span><br><span class="line">early_stopping_cb = keras.callbacks.EarlyStopping(patience=<span class="number">3</span>,</span><br><span class="line">                                                  restore_best_weights=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 원본 epochs = 100</span></span><br><span class="line">history = model.fit(train_seq, train_target, epochs=<span class="number">10</span>, batch_size=<span class="number">64</span>,</span><br><span class="line">                    validation_data=(val_seq, val_target),</span><br><span class="line">                    callbacks=[checkpoint_cb, early_stopping_cb])</span><br></pre></td></tr></table></figure>

<pre><code>Epoch 1/10
313/313 [==============================] - 20s 57ms/step - loss: 0.6929 - accuracy: 0.5167 - val_loss: 0.6923 - val_accuracy: 0.5706
Epoch 2/10
313/313 [==============================] - 15s 46ms/step - loss: 0.6914 - accuracy: 0.6021 - val_loss: 0.6902 - val_accuracy: 0.6312
Epoch 3/10
313/313 [==============================] - 16s 51ms/step - loss: 0.6869 - accuracy: 0.6435 - val_loss: 0.6816 - val_accuracy: 0.5628
Epoch 4/10
313/313 [==============================] - 14s 45ms/step - loss: 0.6530 - accuracy: 0.6578 - val_loss: 0.6313 - val_accuracy: 0.7128
Epoch 5/10
313/313 [==============================] - 13s 42ms/step - loss: 0.6149 - accuracy: 0.7222 - val_loss: 0.6105 - val_accuracy: 0.7282
Epoch 6/10
313/313 [==============================] - 13s 42ms/step - loss: 0.5949 - accuracy: 0.7358 - val_loss: 0.5939 - val_accuracy: 0.7382
Epoch 7/10
313/313 [==============================] - 13s 42ms/step - loss: 0.5780 - accuracy: 0.7503 - val_loss: 0.5786 - val_accuracy: 0.7452
Epoch 8/10
313/313 [==============================] - 13s 43ms/step - loss: 0.5623 - accuracy: 0.7577 - val_loss: 0.5654 - val_accuracy: 0.7466
Epoch 9/10
313/313 [==============================] - 13s 42ms/step - loss: 0.5480 - accuracy: 0.7663 - val_loss: 0.5516 - val_accuracy: 0.7574
Epoch 10/10
313/313 [==============================] - 13s 42ms/step - loss: 0.5343 - accuracy: 0.7711 - val_loss: 0.5400 - val_accuracy: 0.7602
</code></pre>
<h2 id="손실-곡선-추가"><a href="#손실-곡선-추가" class="headerlink" title="손실 곡선 추가"></a>손실 곡선 추가</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">plt.plot(history.history[<span class="string">&#x27;loss&#x27;</span>])</span><br><span class="line">plt.plot(history.history[<span class="string">&#x27;val_loss&#x27;</span>])</span><br><span class="line">plt.xlabel(<span class="string">&#x27;epoch&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;loss&#x27;</span>)</span><br><span class="line">plt.legend([<span class="string">&#x27;train&#x27;</span>, <span class="string">&#x27;val&#x27;</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="/Images/0408_LSTM,GRU_Cell/output_11_0.png" alt="png"></p>
<h2 id="순환층에-드롭아웃-적용하기"><a href="#순환층에-드롭아웃-적용하기" class="headerlink" title="순환층에 드롭아웃 적용하기"></a>순환층에 드롭아웃 적용하기</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">model2 = keras.Sequential()</span><br><span class="line"></span><br><span class="line">model2.add(keras.layers.Embedding(<span class="number">500</span>, <span class="number">16</span>, input_length=<span class="number">100</span>))</span><br><span class="line"><span class="comment"># 드롭아웃 추가</span></span><br><span class="line">model2.add(keras.layers.LSTM(<span class="number">8</span>, dropout=<span class="number">0.3</span>))</span><br><span class="line">model2.add(keras.layers.Dense(<span class="number">1</span>, activation=<span class="string">&#x27;sigmoid&#x27;</span>))</span><br><span class="line"></span><br><span class="line">rmsprop = keras.optimizers.RMSprop(learning_rate=<span class="number">1e-4</span>)</span><br><span class="line">model2.<span class="built_in">compile</span>(optimizer=rmsprop, loss=<span class="string">&#x27;binary_crossentropy&#x27;</span>, </span><br><span class="line">               metrics=[<span class="string">&#x27;accuracy&#x27;</span>])</span><br><span class="line"></span><br><span class="line">checkpoint_cb = keras.callbacks.ModelCheckpoint(<span class="string">&#x27;best-dropout-model.h5&#x27;</span>, </span><br><span class="line">                                                save_best_only=<span class="literal">True</span>)</span><br><span class="line">early_stopping_cb = keras.callbacks.EarlyStopping(patience=<span class="number">3</span>,</span><br><span class="line">                                                  restore_best_weights=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># epcohs = 100</span></span><br><span class="line">history = model2.fit(train_seq, train_target, epochs=<span class="number">5</span>, batch_size=<span class="number">64</span>,</span><br><span class="line">                     validation_data=(val_seq, val_target),</span><br><span class="line">                     callbacks=[checkpoint_cb, early_stopping_cb])</span><br><span class="line"></span><br><span class="line">plt.plot(history.history[<span class="string">&#x27;loss&#x27;</span>])</span><br><span class="line">plt.plot(history.history[<span class="string">&#x27;val_loss&#x27;</span>])</span><br><span class="line">plt.xlabel(<span class="string">&#x27;epoch&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;loss&#x27;</span>)</span><br><span class="line">plt.legend([<span class="string">&#x27;train&#x27;</span>, <span class="string">&#x27;val&#x27;</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<pre><code>Epoch 1/5
313/313 [==============================] - 26s 74ms/step - loss: 0.6925 - accuracy: 0.5351 - val_loss: 0.6917 - val_accuracy: 0.5744
Epoch 2/5
313/313 [==============================] - 14s 44ms/step - loss: 0.6902 - accuracy: 0.5934 - val_loss: 0.6881 - val_accuracy: 0.6306
Epoch 3/5
313/313 [==============================] - 14s 45ms/step - loss: 0.6817 - accuracy: 0.6486 - val_loss: 0.6711 - val_accuracy: 0.6736
Epoch 4/5
313/313 [==============================] - 14s 44ms/step - loss: 0.6357 - accuracy: 0.6889 - val_loss: 0.6024 - val_accuracy: 0.6984
Epoch 5/5
313/313 [==============================] - 14s 45ms/step - loss: 0.5847 - accuracy: 0.7155 - val_loss: 0.5686 - val_accuracy: 0.7286
</code></pre>
<p><img src="/Images/0408_LSTM,GRU_Cell/output_13_1.png" alt="png"></p>
<h1 id="마무리-정리"><a href="#마무리-정리" class="headerlink" title="마무리 정리"></a>마무리 정리</h1><ul>
<li>키워드<ul>
<li><strong>LSTM</strong>: 타임스텝이 긴 데이터를 효과적으로 학습하기 위해 고안된 순환층.</li>
<li><strong>셀 상태</strong>: LSTM셀은 은닉 상태 외에 셀상태를 출력. 셀 상태는 다음 층으로 전달되지 않으며 현재 셀에서만 순환됨.</li>
<li><strong>GPU</strong>: LSTM셀의 간소화 버전으로 생각 할 수 있지만 LSTM셀에 못지않는 성능을 냅니다.</li>
</ul>
</li>
<li><strong>TensorFlow 패키지</strong><ul>
<li><strong>LSTM</strong>: LSTM셀을 사용한 순환층 클래스</li>
<li><strong>GRU</strong>: GRU셀을 사용한 순환층 클래스</li>
</ul>
</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2022-04-08T00:00:00.000Z" title="2022. 4. 8. 오전 9:00:00">2022-04-08</time></span><span class="level-item">Updated&nbsp;<time dateTime="2022-04-08T08:34:23.721Z" title="2022. 4. 8. 오후 5:34:23">2022-04-08</time></span><span class="level-item"> woobin </span><span class="level-item">14 minutes read (About 2079 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/04/08/0408_IMDB/">IMDB리뷰</a></h1><div class="content"><h1 id="순환-신경망으로-IMDB리뷰-분류"><a href="#순환-신경망으로-IMDB리뷰-분류" class="headerlink" title="순환 신경망으로 IMDB리뷰 분류"></a>순환 신경망으로 IMDB리뷰 분류</h1><ul>
<li><p>주제: 긍정리뷰, 부정리뷰 분류하기</p>
<ul>
<li>텍스트 자체는 신경망에 전달하지 않음. (문자열 –&gt; 수식에 적용 X)</li>
<li>문자열을 수식으로 정하는 규칙이 매우 가변적임.</li>
<li>if ex)<ul>
<li><p>He follows the cat. He loves the cat.</p>
</li>
<li><p>10- 11——- 12- 13– 10- 14—- 12- 13</p>
</li>
<li><p>고양이를 따라간다. He follows the cat.</p>
</li>
<li><p>10———– 11———- 12– 13——- 14- 15</p>
</li>
</ul>
</li>
</ul>
</li>
<li><p>RNN, LSTM 알고리즘</p>
<ul>
<li>영어권 사람들이 만듬</li>
<li>자연어 처리와 관련된 많은 알고리즘 또한 영어권 사람들이 만듬</li>
</ul>
</li>
<li><p>한글 !&#x3D; 영어 ( 언어가 다름을 인지 )</p>
<ul>
<li>성과 내려면 제품을 써야함 (&#x3D;돈) (네이버 &#x2F; 카카오쪽 제품)</li>
</ul>
</li>
</ul>
<h1 id="데이터-불러오기"><a href="#데이터-불러오기" class="headerlink" title="데이터 불러오기"></a>데이터 불러오기</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras.datasets <span class="keyword">import</span> imdb</span><br><span class="line">(train_input, train_target), (test_input, test_target) = imdb.load_data(num_words= <span class="number">500</span>)</span><br></pre></td></tr></table></figure>

<pre><code>Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz
17465344/17464789 [==============================] - 0s 0us/step
17473536/17464789 [==============================] - 0s 0us/step
</code></pre>
<ul>
<li>데이터 크기 확인 (1차원 배열)</li>
<li>텍스트의 길이가 다 다르기 때문에 1차원 배열로 정리되어 있음</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(train_input.shape, test_input.shape)</span><br></pre></td></tr></table></figure>

<pre><code>(25000,) (25000,)
</code></pre>
<ul>
<li>문장의 길이 확인<ul>
<li>문장의 길이가 다 다름</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(train_input[<span class="number">0</span>]))</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(train_input[<span class="number">1</span>]))</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(train_input[<span class="number">2</span>]))</span><br></pre></td></tr></table></figure>

<pre><code>218
189
141
</code></pre>
<ul>
<li>Raw 데이터 전처리 -&gt; 토큰화 작업이 끝난 상황 (문자열이 숫자로 바뀜)</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(train_input[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>

<pre><code>[1, 14, 22, 16, 43, 2, 2, 2, 2, 65, 458, 2, 66, 2, 4, 173, 36, 256, 5, 25, 100, 43, 2, 112, 50, 2, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 2, 2, 17, 2, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2, 19, 14, 22, 4, 2, 2, 469, 4, 22, 71, 87, 12, 16, 43, 2, 38, 76, 15, 13, 2, 4, 22, 17, 2, 17, 12, 16, 2, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2, 2, 16, 480, 66, 2, 33, 4, 130, 12, 16, 38, 2, 5, 25, 124, 51, 36, 135, 48, 25, 2, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 2, 5, 2, 36, 71, 43, 2, 476, 26, 400, 317, 46, 7, 4, 2, 2, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 2, 88, 12, 16, 283, 5, 16, 2, 113, 103, 32, 15, 16, 2, 19, 178, 32]
</code></pre>
<ul>
<li>Target 데이터 출력<ul>
<li>0은 부정 리뷰</li>
<li>1은 긍정 리뷰</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 0번째에서 19번째까지의 데이터</span></span><br><span class="line"><span class="built_in">print</span>(train_target[:<span class="number">20</span>])</span><br></pre></td></tr></table></figure>

<pre><code>[1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 0 1]
</code></pre>
<h1 id="데이터셋-분리"><a href="#데이터셋-분리" class="headerlink" title="데이터셋 분리"></a>데이터셋 분리</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">train_input, val_input, train_target, val_target = train_test_split(</span><br><span class="line">    train_input, train_target, test_size= <span class="number">0.2</span>, random_state= <span class="number">42</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_input.shape, train_target.shape, val_input.shape, val_target.shape</span><br></pre></td></tr></table></figure>




<pre><code>((20000,), (20000,), (5000,), (5000,))
</code></pre>
<h1 id="데이터-시각화"><a href="#데이터-시각화" class="headerlink" title="데이터 시각화"></a>데이터 시각화</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment">#  컴프리헨션으로 길이를 표현하는 배열 만들기</span></span><br><span class="line">lengths= np.array([<span class="built_in">len</span>(x) <span class="keyword">for</span> x <span class="keyword">in</span> train_input])</span><br><span class="line"><span class="built_in">print</span>(np.mean(lengths), np.median(lengths), np.<span class="built_in">max</span>(lengths))</span><br></pre></td></tr></table></figure>

<pre><code>238.71364 178.0 2494
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.hist(lengths)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="/Images/0408_IMDB/output_17_0.png" alt="png"></p>
<pre><code>- 250이하의 길이가 대부분임을 알 수 있음.
</code></pre>
<ul>
<li>짧은 단어 100개만 사용 할 예정</li>
<li>모든 길이를 100에 맞추는 ‘패딩’작업 실행</li>
<li>데이터의 갯수는 20000, 전체 길이는 100으로 맞춤</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras.preprocessing.sequence <span class="keyword">import</span> pad_sequences</span><br><span class="line">train_seq= pad_sequences(train_input, maxlen= <span class="number">100</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(train_seq.shape)</span><br></pre></td></tr></table></figure>

<pre><code>(25000, 100)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">len</span>(train_input[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>




<pre><code>218
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(train_seq[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>

<pre><code>[  2  33   6  22  12 215  28  77  52   5  14 407  16  82   2   8   4 107
 117   2  15 256   4   2   7   2   5   2  36  71  43   2 476  26 400 317
  46   7   4   2   2  13 104  88   4 381  15 297  98  32   2  56  26 141
   6 194   2  18   4 226  22  21 134 476  26 480   5 144  30   2  18  51
  36  28 224  92  25 104   4 226  65  16  38   2  88  12  16 283   5  16
   2 113 103  32  15  16   2  19 178  32]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(train_input[<span class="number">0</span>][-<span class="number">10</span>:])  <span class="comment"># 음수의 인덱스로 슬라이싱 하면 끝에서부터 세기 시작함.</span></span><br><span class="line"><span class="built_in">print</span>()</span><br><span class="line"><span class="built_in">print</span>(train_input[<span class="number">0</span>][<span class="number">208</span>:])</span><br></pre></td></tr></table></figure>

<pre><code>[2, 113, 103, 32, 15, 16, 2, 19, 178, 32]

[2, 113, 103, 32, 15, 16, 2, 19, 178, 32]


- train_seq[0]의 길이는 218로 100보다 길기때문에 패딩이 안되었음을 확인 할 수 있음
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val_seq = pad_sequences(val_input, maxlen= <span class="number">100</span>)</span><br><span class="line">val_seq</span><br></pre></td></tr></table></figure>




<pre><code>array([[ 32,   2, 225, ...,  14,  58,   2],
       [ 53,   2,   8, ...,   7,  32,   2],
       [  0,   0,   0, ...,   2,  33,  32],
       ...,
       [383,   2, 120, ...,  16,  99,  76],
       [106, 345,  12, ..., 120,   2, 156],
       [  4, 114,  21, ...,   4,   2,   2]], dtype=int32)
</code></pre>
<h1 id="순환신경망-만들기"><a href="#순환신경망-만들기" class="headerlink" title="순환신경망 만들기"></a>순환신경망 만들기</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br><span class="line">model = keras.Sequential()</span><br><span class="line"><span class="comment"># 최대 길이는 100까지 패딩, imdb.load_data() 함수에서 500개의 단어만 사용하도록 지정했었음</span></span><br><span class="line">model.add(keras.layers.SimpleRNN(<span class="number">8</span>, input_shape= (<span class="number">100</span>, <span class="number">500</span>)))</span><br><span class="line">model.add(keras.layers.Dense(<span class="number">1</span>, activation= <span class="string">&#x27;sigmoid&#x27;</span>))</span><br></pre></td></tr></table></figure>

<ul>
<li>원핫인코딩 적용</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># to_categorical(): 원핫인코딩된 배열로 반환해줌.</span></span><br><span class="line">train_oh = keras.utils.to_categorical(train_seq)</span><br><span class="line"><span class="built_in">print</span>(train_oh.shape)</span><br></pre></td></tr></table></figure>

<pre><code>(20000, 100, 500)


- 정수 하나마다 500차원의 (20000, 100)의 크기로 들어가있음을 확인 할 수 있음
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(train_oh[<span class="number">0</span>][<span class="number">0</span>][:<span class="number">12</span>])</span><br></pre></td></tr></table></figure>

<pre><code>[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 나머지 원소에 전부 0으로 들어가있는지 확인</span></span><br><span class="line"><span class="built_in">print</span>(np.<span class="built_in">sum</span>(train_oh[<span class="number">0</span>][<span class="number">0</span>]))</span><br></pre></td></tr></table></figure>

<pre><code>1.0
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val_oh = keras.utils.to_categorical(val_seq)</span><br><span class="line"><span class="built_in">print</span>(val_oh.shape)</span><br></pre></td></tr></table></figure>

<pre><code>(5000, 100, 500)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.summary()</span><br></pre></td></tr></table></figure>

<pre><code>Model: &quot;sequential_3&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 simple_rnn_3 (SimpleRNN)    (None, 8)                 4072      
                                                                 
 dense_3 (Dense)             (None, 1)                 9         
                                                                 
=================================================================
Total params: 4,081
Trainable params: 4,081
Non-trainable params: 0
_________________________________________________________________
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">rmsprop = keras.optimizers.RMSprop(learning_rate=<span class="number">1e-4</span>)</span><br><span class="line">model.<span class="built_in">compile</span>(optimizer=rmsprop, loss=<span class="string">&#x27;binary_crossentropy&#x27;</span>, </span><br><span class="line">              metrics=[<span class="string">&#x27;accuracy&#x27;</span>])</span><br><span class="line"></span><br><span class="line">checkpoint_cb = keras.callbacks.ModelCheckpoint(<span class="string">&#x27;best-simplernn-model.h5&#x27;</span>, </span><br><span class="line">                                                save_best_only=<span class="literal">True</span>)</span><br><span class="line">early_stopping_cb = keras.callbacks.EarlyStopping(patience=<span class="number">3</span>,</span><br><span class="line">                                                  restore_best_weights=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 원본 epochs = 100</span></span><br><span class="line">history = model.fit(train_oh, train_target, epochs=<span class="number">100</span>, batch_size=<span class="number">64</span>,</span><br><span class="line">                    validation_data=(val_oh, val_target),</span><br><span class="line">                    callbacks=[checkpoint_cb, early_stopping_cb])</span><br></pre></td></tr></table></figure>

<pre><code>Epoch 1/100
313/313 [==============================] - 15s 44ms/step - loss: 0.5339 - accuracy: 0.7710 - val_loss: 0.5444 - val_accuracy: 0.7566
Epoch 2/100
313/313 [==============================] - 11s 36ms/step - loss: 0.5241 - accuracy: 0.7762 - val_loss: 0.5373 - val_accuracy: 0.7528
Epoch 3/100
313/313 [==============================] - 12s 37ms/step - loss: 0.5160 - accuracy: 0.7796 - val_loss: 0.5296 - val_accuracy: 0.7576
Epoch 4/100
313/313 [==============================] - 11s 36ms/step - loss: 0.5079 - accuracy: 0.7828 - val_loss: 0.5231 - val_accuracy: 0.7592
Epoch 5/100
313/313 [==============================] - 12s 37ms/step - loss: 0.4998 - accuracy: 0.7872 - val_loss: 0.5165 - val_accuracy: 0.7660
Epoch 6/100
313/313 [==============================] - 11s 36ms/step - loss: 0.4931 - accuracy: 0.7887 - val_loss: 0.5115 - val_accuracy: 0.7666
Epoch 7/100
313/313 [==============================] - 12s 37ms/step - loss: 0.4858 - accuracy: 0.7914 - val_loss: 0.5061 - val_accuracy: 0.7710
Epoch 8/100
313/313 [==============================] - 11s 36ms/step - loss: 0.4801 - accuracy: 0.7949 - val_loss: 0.5015 - val_accuracy: 0.7690
Epoch 9/100
313/313 [==============================] - 11s 37ms/step - loss: 0.4742 - accuracy: 0.7972 - val_loss: 0.4985 - val_accuracy: 0.7684
Epoch 10/100
313/313 [==============================] - 12s 38ms/step - loss: 0.4688 - accuracy: 0.7982 - val_loss: 0.4931 - val_accuracy: 0.7744
Epoch 11/100
313/313 [==============================] - 16s 50ms/step - loss: 0.4636 - accuracy: 0.8023 - val_loss: 0.4941 - val_accuracy: 0.7674
Epoch 12/100
313/313 [==============================] - 11s 36ms/step - loss: 0.4595 - accuracy: 0.8023 - val_loss: 0.4893 - val_accuracy: 0.7732
Epoch 13/100
313/313 [==============================] - 11s 36ms/step - loss: 0.4546 - accuracy: 0.8051 - val_loss: 0.4850 - val_accuracy: 0.7782
Epoch 14/100
313/313 [==============================] - 11s 36ms/step - loss: 0.4512 - accuracy: 0.8065 - val_loss: 0.4837 - val_accuracy: 0.7760
Epoch 15/100
313/313 [==============================] - 11s 36ms/step - loss: 0.4471 - accuracy: 0.8097 - val_loss: 0.4826 - val_accuracy: 0.7766
Epoch 16/100
313/313 [==============================] - 11s 36ms/step - loss: 0.4438 - accuracy: 0.8109 - val_loss: 0.4789 - val_accuracy: 0.7796
Epoch 17/100
313/313 [==============================] - 11s 36ms/step - loss: 0.4396 - accuracy: 0.8121 - val_loss: 0.4795 - val_accuracy: 0.7764
Epoch 18/100
313/313 [==============================] - 12s 38ms/step - loss: 0.4367 - accuracy: 0.8133 - val_loss: 0.4792 - val_accuracy: 0.7760
Epoch 19/100
313/313 [==============================] - 12s 40ms/step - loss: 0.4344 - accuracy: 0.8127 - val_loss: 0.4761 - val_accuracy: 0.7784
Epoch 20/100
313/313 [==============================] - 15s 47ms/step - loss: 0.4313 - accuracy: 0.8157 - val_loss: 0.4733 - val_accuracy: 0.7822
Epoch 21/100
313/313 [==============================] - 11s 37ms/step - loss: 0.4289 - accuracy: 0.8162 - val_loss: 0.4753 - val_accuracy: 0.7794
Epoch 22/100
313/313 [==============================] - 11s 35ms/step - loss: 0.4261 - accuracy: 0.8173 - val_loss: 0.4757 - val_accuracy: 0.7806
Epoch 23/100
313/313 [==============================] - 11s 36ms/step - loss: 0.4241 - accuracy: 0.8167 - val_loss: 0.4717 - val_accuracy: 0.7826
Epoch 24/100
313/313 [==============================] - 11s 36ms/step - loss: 0.4220 - accuracy: 0.8188 - val_loss: 0.4735 - val_accuracy: 0.7816
Epoch 25/100
313/313 [==============================] - 12s 37ms/step - loss: 0.4197 - accuracy: 0.8202 - val_loss: 0.4696 - val_accuracy: 0.7820
Epoch 26/100
313/313 [==============================] - 11s 36ms/step - loss: 0.4179 - accuracy: 0.8201 - val_loss: 0.4728 - val_accuracy: 0.7834
Epoch 27/100
313/313 [==============================] - 11s 36ms/step - loss: 0.4160 - accuracy: 0.8210 - val_loss: 0.4736 - val_accuracy: 0.7830
Epoch 28/100
313/313 [==============================] - 11s 36ms/step - loss: 0.4142 - accuracy: 0.8209 - val_loss: 0.4711 - val_accuracy: 0.7836
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(history.history[<span class="string">&#x27;loss&#x27;</span>])</span><br><span class="line">plt.plot(history.history[<span class="string">&#x27;val_loss&#x27;</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="/Images/0408_IMDB/output_37_0.png" alt="png"></p>
<h1 id="단어-임베딩을-사용"><a href="#단어-임베딩을-사용" class="headerlink" title="단어 임베딩을 사용"></a>단어 임베딩을 사용</h1><ul>
<li>문제점 발생: 토큰 1개를 500차원으로 늘림<ul>
<li>-&gt; 데이터 크기가 500배 커짐</li>
</ul>
</li>
<li>단어임베딩: 라벨인코딩과 비슷</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br><span class="line">model2 = keras.Sequential()</span><br><span class="line">model2.add(keras.layers.Embedding(<span class="number">500</span>, <span class="number">16</span>, input_length= <span class="number">100</span>))</span><br><span class="line">model2.add(keras.layers.SimpleRNN(<span class="number">8</span>))</span><br><span class="line">model2.add(keras.layers.Dense(<span class="number">1</span>, activation= <span class="string">&#x27;sigmoid&#x27;</span>))</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model2.summary()</span><br></pre></td></tr></table></figure>

<pre><code>Model: &quot;sequential_4&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 embedding_2 (Embedding)     (None, 100, 16)           8000      
                                                                 
 simple_rnn_4 (SimpleRNN)    (None, 8)                 200       
                                                                 
 dense_4 (Dense)             (None, 1)                 9         
                                                                 
=================================================================
Total params: 8,209
Trainable params: 8,209
Non-trainable params: 0
_________________________________________________________________
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">rmsprop = keras.optimizers.RMSprop(learning_rate=<span class="number">1e-4</span>)</span><br><span class="line">model2.<span class="built_in">compile</span>(optimizer=rmsprop, loss=<span class="string">&#x27;binary_crossentropy&#x27;</span>, </span><br><span class="line">              metrics=[<span class="string">&#x27;accuracy&#x27;</span>])</span><br><span class="line"><span class="comment"># 임베딩으로 Change</span></span><br><span class="line">checkpoint_cb = keras.callbacks.ModelCheckpoint(<span class="string">&#x27;best-embedding-model.h5&#x27;</span>, </span><br><span class="line">                                                save_best_only=<span class="literal">True</span>)</span><br><span class="line">early_stopping_cb = keras.callbacks.EarlyStopping(patience=<span class="number">3</span>,</span><br><span class="line">                                                  restore_best_weights=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 원본 epochs= 100</span></span><br><span class="line">history = model2.fit(train_seq, train_target, epochs=<span class="number">100</span>, batch_size=<span class="number">64</span>,</span><br><span class="line">                    validation_data=(val_seq, val_target),</span><br><span class="line">                    callbacks=[checkpoint_cb, early_stopping_cb])</span><br></pre></td></tr></table></figure>

<pre><code>Epoch 1/100
313/313 [==============================] - 8s 22ms/step - loss: 0.6812 - accuracy: 0.5584 - val_loss: 0.6570 - val_accuracy: 0.6208
Epoch 2/100
313/313 [==============================] - 6s 19ms/step - loss: 0.6334 - accuracy: 0.6675 - val_loss: 0.6146 - val_accuracy: 0.6916
Epoch 3/100
313/313 [==============================] - 6s 19ms/step - loss: 0.5940 - accuracy: 0.7219 - val_loss: 0.5831 - val_accuracy: 0.7314
Epoch 4/100
313/313 [==============================] - 6s 19ms/step - loss: 0.5655 - accuracy: 0.7492 - val_loss: 0.5596 - val_accuracy: 0.7450
Epoch 5/100
313/313 [==============================] - 7s 21ms/step - loss: 0.5455 - accuracy: 0.7593 - val_loss: 0.5461 - val_accuracy: 0.7518
Epoch 6/100
313/313 [==============================] - 6s 20ms/step - loss: 0.5302 - accuracy: 0.7678 - val_loss: 0.5405 - val_accuracy: 0.7446
Epoch 7/100
313/313 [==============================] - 6s 19ms/step - loss: 0.5170 - accuracy: 0.7740 - val_loss: 0.5327 - val_accuracy: 0.7492
Epoch 8/100
313/313 [==============================] - 7s 21ms/step - loss: 0.5076 - accuracy: 0.7765 - val_loss: 0.5195 - val_accuracy: 0.7598
Epoch 9/100
313/313 [==============================] - 6s 19ms/step - loss: 0.4996 - accuracy: 0.7814 - val_loss: 0.5141 - val_accuracy: 0.7592
Epoch 10/100
313/313 [==============================] - 6s 20ms/step - loss: 0.4908 - accuracy: 0.7845 - val_loss: 0.5085 - val_accuracy: 0.7642
Epoch 11/100
313/313 [==============================] - 6s 20ms/step - loss: 0.4844 - accuracy: 0.7876 - val_loss: 0.5082 - val_accuracy: 0.7598
Epoch 12/100
313/313 [==============================] - 6s 20ms/step - loss: 0.4782 - accuracy: 0.7904 - val_loss: 0.4958 - val_accuracy: 0.7722
Epoch 13/100
313/313 [==============================] - 6s 21ms/step - loss: 0.4711 - accuracy: 0.7937 - val_loss: 0.4989 - val_accuracy: 0.7644
Epoch 14/100
313/313 [==============================] - 6s 20ms/step - loss: 0.4640 - accuracy: 0.7980 - val_loss: 0.4876 - val_accuracy: 0.7740
Epoch 15/100
313/313 [==============================] - 6s 19ms/step - loss: 0.4607 - accuracy: 0.7991 - val_loss: 0.4833 - val_accuracy: 0.7770
Epoch 16/100
313/313 [==============================] - 6s 20ms/step - loss: 0.4541 - accuracy: 0.8021 - val_loss: 0.4896 - val_accuracy: 0.7682
Epoch 17/100
313/313 [==============================] - 6s 20ms/step - loss: 0.4496 - accuracy: 0.8054 - val_loss: 0.4753 - val_accuracy: 0.7810
Epoch 18/100
313/313 [==============================] - 6s 20ms/step - loss: 0.4458 - accuracy: 0.8079 - val_loss: 0.4748 - val_accuracy: 0.7790
Epoch 19/100
313/313 [==============================] - 6s 19ms/step - loss: 0.4421 - accuracy: 0.8102 - val_loss: 0.4753 - val_accuracy: 0.7786
Epoch 20/100
313/313 [==============================] - 6s 19ms/step - loss: 0.4390 - accuracy: 0.8127 - val_loss: 0.4717 - val_accuracy: 0.7772
Epoch 21/100
313/313 [==============================] - 6s 19ms/step - loss: 0.4351 - accuracy: 0.8131 - val_loss: 0.4823 - val_accuracy: 0.7732
Epoch 22/100
313/313 [==============================] - 6s 20ms/step - loss: 0.4331 - accuracy: 0.8137 - val_loss: 0.4659 - val_accuracy: 0.7842
Epoch 23/100
313/313 [==============================] - 7s 22ms/step - loss: 0.4301 - accuracy: 0.8145 - val_loss: 0.4655 - val_accuracy: 0.7812
Epoch 24/100
313/313 [==============================] - 6s 20ms/step - loss: 0.4260 - accuracy: 0.8174 - val_loss: 0.4771 - val_accuracy: 0.7754
Epoch 25/100
313/313 [==============================] - 6s 21ms/step - loss: 0.4243 - accuracy: 0.8185 - val_loss: 0.4637 - val_accuracy: 0.7846
Epoch 26/100
313/313 [==============================] - 6s 19ms/step - loss: 0.4208 - accuracy: 0.8205 - val_loss: 0.4658 - val_accuracy: 0.7834
Epoch 27/100
313/313 [==============================] - 6s 20ms/step - loss: 0.4185 - accuracy: 0.8222 - val_loss: 0.4652 - val_accuracy: 0.7820
Epoch 28/100
313/313 [==============================] - 6s 20ms/step - loss: 0.4161 - accuracy: 0.8236 - val_loss: 0.4609 - val_accuracy: 0.7866
Epoch 29/100
313/313 [==============================] - 7s 21ms/step - loss: 0.4136 - accuracy: 0.8249 - val_loss: 0.4642 - val_accuracy: 0.7860
Epoch 30/100
313/313 [==============================] - 6s 19ms/step - loss: 0.4113 - accuracy: 0.8254 - val_loss: 0.4601 - val_accuracy: 0.7876
Epoch 31/100
313/313 [==============================] - 6s 20ms/step - loss: 0.4090 - accuracy: 0.8281 - val_loss: 0.4643 - val_accuracy: 0.7824
Epoch 32/100
313/313 [==============================] - 6s 20ms/step - loss: 0.4076 - accuracy: 0.8286 - val_loss: 0.4622 - val_accuracy: 0.7874
Epoch 33/100
313/313 [==============================] - 6s 19ms/step - loss: 0.4053 - accuracy: 0.8289 - val_loss: 0.4590 - val_accuracy: 0.7882
Epoch 34/100
313/313 [==============================] - 6s 21ms/step - loss: 0.4030 - accuracy: 0.8299 - val_loss: 0.4572 - val_accuracy: 0.7902
Epoch 35/100
313/313 [==============================] - 7s 21ms/step - loss: 0.4010 - accuracy: 0.8321 - val_loss: 0.4632 - val_accuracy: 0.7878
Epoch 36/100
313/313 [==============================] - 6s 19ms/step - loss: 0.3995 - accuracy: 0.8331 - val_loss: 0.4571 - val_accuracy: 0.7914
Epoch 37/100
313/313 [==============================] - 6s 19ms/step - loss: 0.3973 - accuracy: 0.8347 - val_loss: 0.4621 - val_accuracy: 0.7892
Epoch 38/100
313/313 [==============================] - 7s 21ms/step - loss: 0.3958 - accuracy: 0.8353 - val_loss: 0.4568 - val_accuracy: 0.7916
Epoch 39/100
313/313 [==============================] - 6s 20ms/step - loss: 0.3941 - accuracy: 0.8354 - val_loss: 0.4626 - val_accuracy: 0.7868
Epoch 40/100
313/313 [==============================] - 7s 22ms/step - loss: 0.3927 - accuracy: 0.8358 - val_loss: 0.4611 - val_accuracy: 0.7886
Epoch 41/100
313/313 [==============================] - 6s 20ms/step - loss: 0.3907 - accuracy: 0.8371 - val_loss: 0.4583 - val_accuracy: 0.7912
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(history.history[<span class="string">&#x27;loss&#x27;</span>])</span><br><span class="line">plt.plot(history.history[<span class="string">&#x27;val_loss&#x27;</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="/Images/0408_IMDB/output_42_0.png" alt="png"></p>
<h1 id="마무리-정리"><a href="#마무리-정리" class="headerlink" title="마무리 정리"></a>마무리 정리</h1><ul>
<li><p>키워드</p>
<ul>
<li><strong>말뭉치</strong>: 자연어 처리에서 사용하는 텍스트 데이터의 모음. (&#x3D; 훈련 데이터셋)</li>
<li><strong>토큰</strong>: 텍스트에서 공백으로 구분되는 문자열 종종 소문자로 변환하고 구둣점은 삭제</li>
<li><strong>원-핫 인코딩</strong>: 어떤 클래스에 해당하는 원소만 1이고 나머지는 모두 0인 벡터</li>
<li><strong>단어 임베딩</strong>: 정수로 변환된 토큰을 비교적 작은 크기의 실수 밀집 벡터로 변환. 이런 밀집 벡터는 단어 사이의 관계를 표현 할 수 있기 때문에 자연어 처리에서 좋은 성능을 발휘</li>
</ul>
</li>
<li><p><strong>TensorFlow 패키지</strong></p>
<ul>
<li><strong>pad_sequences()</strong>: 시퀀스 길이를 맞추기 위해 패딩을 추가. (샘플 개수, 타임스텝 개수)크기의 2차원 배열로 나옴.<ul>
<li>매개변수 maxlen: 원하는 시퀀스 길이를 지정. 지정한 값보다 긴 시퀀스는 잘리고, 짧은 시퀀스는 패딩됨.</li>
</ul>
</li>
<li><strong>to_categorical()</strong>: 정수 시퀀스를 원 핫 인코딩으로 변환. 토큰이나 타깃값을 원핫 인코딩 할 때 사용</li>
<li><strong>SumpleRNN</strong>: 케라스의 기본 순환층 클래스</li>
<li><strong>Embedding</strong>: 단어 임베딩을 위한 클래스</li>
</ul>
</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2022-04-07T00:00:00.000Z" title="2022. 4. 7. 오전 9:00:00">2022-04-07</time></span><span class="level-item">Updated&nbsp;<time dateTime="2022-04-07T07:59:31.522Z" title="2022. 4. 7. 오후 4:59:31">2022-04-07</time></span><span class="level-item"> woobin </span><span class="level-item">3 minutes read (About 463 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/04/07/0407_Sequentoal_Data,Recurrent_Neural_Network/">순차 데이터와 순환 신경망</a></h1><div class="content"><h1 id="순차-데이터와-순환-신경망"><a href="#순차-데이터와-순환-신경망" class="headerlink" title="순차 데이터와 순환 신경망"></a>순차 데이터와 순환 신경망</h1><ul>
<li>초급 레벨: 기초통계 (t.test, 분산분석, 회귀분석…)</li>
<li>중급 레벨: 시계열 분석 &#x2F; 베이지안 &#x2F; 비모수검정…</li>
</ul>
<h1 id="순차-데이터"><a href="#순차-데이터" class="headerlink" title="순차 데이터"></a>순차 데이터</h1><ul>
<li>텍스트나 시계열 데이터와 같이 순서에 의미가 있는 데이터를 뜻함<ul>
<li>시계열 데이터: 주식 &#x2F; 날씨 &#x2F; 매장 매출 등등<ul>
<li>시계열 데이터를 공부하고 싶다면 R 로 공부할 것 권장</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1 id="텍스트"><a href="#텍스트" class="headerlink" title="텍스트"></a>텍스트</h1><ul>
<li><p>텍스트 마이닝 ( 데이터 분석가 )</p>
<ul>
<li>대표적으로 감정분석 ( 긍정, 부정 )</li>
<li>문자열: 인코딩하는 방법론이 존재</li>
</ul>
</li>
<li><p>자연어 처리 ( 개발자 )</p>
<ul>
<li>챗봇 ( 툴은 다 존재함 )</li>
<li>자동 번역</li>
</ul>
</li>
<li><p>기본 딥러닝 알고리즘 &#x2F; RNN &amp; LSTM</p>
<ul>
<li>현실에서 사용하지는 않음.</li>
</ul>
</li>
<li><p>자료</p>
<ul>
<li>딥러닝을 이용한 자연어 처리 입문 (텐서플로) : <a target="_blank" rel="noopener" href="https://wikidocs.net/book/2155">https://wikidocs.net/book/2155</a></li>
<li>Pytorch로 시작하는 딥러닝 입문 : <a target="_blank" rel="noopener" href="https://wikidocs.net/32471">https://wikidocs.net/32471</a></li>
</ul>
</li>
<li><p>딥러닝은 <strong>분야 선정</strong>이 중요</p>
<ul>
<li>영상인식, 이미지 분류, 음성, 자연어</li>
</ul>
</li>
</ul>
<h1 id="순환-신경망"><a href="#순환-신경망" class="headerlink" title="순환 신경망"></a>순환 신경망</h1><ul>
<li><p>일반적으로 완전 연결 신경망과 거의 비슷하지만, 완전 연결 신경망에 이전 데이터의 처리 흐름을 순환하는 고리 하나를 추가하는 형태</p>
</li>
<li><p>이미지는 픽셀값이 어느정도 고정이 되어있음</p>
<ul>
<li>if) 28 * 28 로 정의 –&gt; 모든 데이터를 28 * 28 로 맞출 수 있음</li>
</ul>
</li>
<li><p>텍스트</p>
<ul>
<li>값이 고정이 불가함</li>
<li>같은 의미</li>
</ul>
</li>
<li><p>p.494</p>
<ul>
<li>ex) i am boy (1, 4, 3)</li>
</ul>
</li>
</ul>
<h1 id="마무리-정리"><a href="#마무리-정리" class="headerlink" title="마무리 정리"></a>마무리 정리</h1><ul>
<li><strong>순차 데이터</strong>: 텍스트나 시계열 데이터와 같이 순서에 의미가 있는 데이터</li>
<li><strong>순환 신경망</strong>: 순차 데이터에 잘 맞는 인공 신경망의 한 종류</li>
<li><strong>셀</strong>: 순환 신경망에서의 순환층</li>
<li><strong>은닉 상태</strong>: 순환 신경망에서의 셀의 출력</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2022-04-07T00:00:00.000Z" title="2022. 4. 7. 오전 9:00:00">2022-04-07</time></span><span class="level-item">Updated&nbsp;<time dateTime="2022-04-08T05:00:41.400Z" title="2022. 4. 8. 오후 2:00:41">2022-04-08</time></span><span class="level-item"> woobin </span><span class="level-item">15 minutes read (About 2185 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/04/07/0407_Convolution_Neural_Network_02/">합성곱 신경망_02</a></h1><div class="content"><h1 id="합성곱-신경망을-사용한-이미지-분류"><a href="#합성곱-신경망을-사용한-이미지-분류" class="headerlink" title="합성곱 신경망을 사용한 이미지 분류"></a>합성곱 신경망을 사용한 이미지 분류</h1><h1 id="패션-MNIST-데이터-불러오기"><a href="#패션-MNIST-데이터-불러오기" class="headerlink" title="패션 MNIST 데이터 불러오기"></a>패션 MNIST 데이터 불러오기</h1><ul>
<li><p>데이터 스케일 0 ~ 255의 데이터를 0 ~ 1로 표준화</p>
</li>
<li><p>훈련 데이터 &#x2F; 검증 데이터 분류</p>
</li>
<li><p>합성곱 신경망은 2차원 이미지를 그대로 사용하기 때문에 일렬로 펼치지 않아도 됨</p>
</li>
<li><p>완전 연결 신경망 (Fully Connected Layer)</p>
<ul>
<li>2차원 배열을 1차원 배열로 바꿔야함</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line">(train_input, train_target), (test_input, test_target) = \</span><br><span class="line">    keras.datasets.fashion_mnist.load_data()</span><br><span class="line"></span><br><span class="line">train_scaled = train_input.reshape(-<span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>) / <span class="number">255.0</span></span><br><span class="line"></span><br><span class="line">train_scaled, val_scaled, train_target, val_target = train_test_split(</span><br><span class="line">    train_scaled, train_target, test_size=<span class="number">0.2</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(train_input.shape, train_scaled.shape)</span><br></pre></td></tr></table></figure>

<pre><code>Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz
32768/29515 [=================================] - 0s 0us/step
40960/29515 [=========================================] - 0s 0us/step
Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz
26427392/26421880 [==============================] - 0s 0us/step
26435584/26421880 [==============================] - 0s 0us/step
Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz
16384/5148 [===============================================================================================] - 0s 0us/step
Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz
4423680/4422102 [==============================] - 0s 0us/step
4431872/4422102 [==============================] - 0s 0us/step
(60000, 28, 28) (48000, 28, 28, 1)


- (60000, 28, 28)크기였던 train_input이 (48000, 28, 28, 1)크기의 train_scaled가 됨.
</code></pre>
<h1 id="합성곱-신경망-만들기"><a href="#합성곱-신경망-만들기" class="headerlink" title="합성곱 신경망 만들기"></a>합성곱 신경망 만들기</h1><ul>
<li>합성곱 신경망의 전형적인 구조<ul>
<li>합성곱 층으로 이미지에서 특징을 감지 한 후 밀집층으로 클래스에 따른 분류 확률을 계산</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 실험 코드</span></span><br><span class="line">model = keras.Sequential()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 합성곱 층</span></span><br><span class="line">model.add(keras.layers.Conv2D(<span class="number">32</span>, kernel_size = <span class="number">3</span>, activation = <span class="string">&#x27;relu&#x27;</span>,</span><br><span class="line">                              padding = <span class="string">&#x27;same&#x27;</span>, input_shape = (<span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 풀링층</span></span><br><span class="line"><span class="comment"># 평균 풀링을 사용, 풀링 크기는 (3, 3)로 지정</span></span><br><span class="line"><span class="comment"># # 최대 풀링의 함수: MaxPooling2D()</span></span><br><span class="line"><span class="comment"># # 평균 풀링의 함수: AveragePooling2D()</span></span><br><span class="line">model.add(keras.layers.AveragePooling2D(<span class="number">3</span>))  <span class="comment"># 1/3으로 줄이겠다는 의미</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 합성곱 층</span></span><br><span class="line">model.add(keras.layers.Conv2D(<span class="number">32</span>, kernel_size=(<span class="number">3</span>,<span class="number">3</span>), activation=<span class="string">&#x27;relu&#x27;</span>, </span><br><span class="line">                              padding=<span class="string">&#x27;same&#x27;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 풀링층</span></span><br><span class="line">model.add(keras.layers.AveragePooling2D(<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 완전연결층 (밀집층 = Fully Connected Layer)</span></span><br><span class="line">model.add(keras.layers.Flatten())</span><br><span class="line">model.add(keras.layers.Dense(<span class="number">100</span>, activation=<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line">model.add(keras.layers.Dropout(<span class="number">0.4</span>))</span><br><span class="line">model.add(keras.layers.Dense(<span class="number">10</span>, activation=<span class="string">&#x27;softmax&#x27;</span>))</span><br><span class="line"></span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure>

<pre><code>Model: &quot;sequential&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 conv2d (Conv2D)             (None, 28, 28, 32)        320       
                                                                 
 average_pooling2d (AverageP  (None, 9, 9, 32)         0         
 ooling2D)                                                       
                                                                 
 conv2d_1 (Conv2D)           (None, 9, 9, 32)          9248      
                                                                 
 average_pooling2d_1 (Averag  (None, 3, 3, 32)         0         
 ePooling2D)                                                     
                                                                 
 flatten (Flatten)           (None, 288)               0         
                                                                 
 dense (Dense)               (None, 100)               28900     
                                                                 
 dropout (Dropout)           (None, 100)               0         
                                                                 
 dense_1 (Dense)             (None, 10)                1010      
                                                                 
=================================================================
Total params: 39,478
Trainable params: 39,478
Non-trainable params: 0
_________________________________________________________________


                                        
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">model = keras.Sequential()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 합성곱 층</span></span><br><span class="line">model.add(keras.layers.Conv2D(<span class="number">32</span>, kernel_size = <span class="number">3</span>, activation = <span class="string">&#x27;relu&#x27;</span>,</span><br><span class="line">                              padding = <span class="string">&#x27;same&#x27;</span>, input_shape = (<span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 풀링층</span></span><br><span class="line"><span class="comment"># 최대 풀링을 사용, 전형적인 풀링 크기인 (2,2)로 지정</span></span><br><span class="line"><span class="comment"># # 최대 풀링의 함수: MaxPooling2D()</span></span><br><span class="line"><span class="comment"># # 평균 풀링의 함수: AveragePooling2D()</span></span><br><span class="line">model.add(keras.layers.MaxPooling2D(<span class="number">2</span>))  <span class="comment"># 절반으로 줄이겠다는 의미</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 합성곱 층</span></span><br><span class="line">model.add(keras.layers.Conv2D(<span class="number">64</span>, kernel_size=(<span class="number">3</span>,<span class="number">3</span>), activation=<span class="string">&#x27;relu&#x27;</span>, </span><br><span class="line">                              padding=<span class="string">&#x27;same&#x27;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 풀링층</span></span><br><span class="line">model.add(keras.layers.MaxPooling2D(<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 완전연결층 (밀집층 = Fully Connected Layer)</span></span><br><span class="line">model.add(keras.layers.Flatten())</span><br><span class="line">model.add(keras.layers.Dense(<span class="number">100</span>, activation=<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line">model.add(keras.layers.Dropout(<span class="number">0.4</span>))</span><br><span class="line">model.add(keras.layers.Dense(<span class="number">10</span>, activation=<span class="string">&#x27;softmax&#x27;</span>))</span><br><span class="line"></span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure>

<pre><code>Model: &quot;sequential_1&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 conv2d_2 (Conv2D)           (None, 28, 28, 32)        320       
                                                                 
 max_pooling2d (MaxPooling2D  (None, 14, 14, 32)       0         
 )                                                               
                                                                 
 conv2d_3 (Conv2D)           (None, 14, 14, 64)        18496     
                                                                 
 max_pooling2d_1 (MaxPooling  (None, 7, 7, 64)         0         
 2D)                                                             
                                                                 
 flatten_1 (Flatten)         (None, 3136)              0         
                                                                 
 dense_2 (Dense)             (None, 100)               313700    
                                                                 
 dropout_1 (Dropout)         (None, 100)               0         
                                                                 
 dense_3 (Dense)             (None, 10)                1010      
                                                                 
=================================================================
Total params: 333,526
Trainable params: 333,526
Non-trainable params: 0
_________________________________________________________________
</code></pre>
<ul>
<li>층의 구조를 그림으로 표현<ul>
<li>plot_model()함수</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.utils.plot_model(model)</span><br></pre></td></tr></table></figure>




<p><img src="/Images/0407_Convolution_Neural_Network_02/output_9_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 실험 코드</span></span><br><span class="line">keras.utils.plot_model(model, show_shapes=<span class="literal">True</span>,</span><br><span class="line">                       dpi= <span class="number">50</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># dpi로 크기를 조절 할 수 있음 [default: 96]</span></span><br></pre></td></tr></table></figure>




<p><img src="/Images/0407_Convolution_Neural_Network_02/output_10_0.png" alt="png"></p>
<ul>
<li>매개변수 show_shapes를 적용하여 입력과 출력의 크기를 표시</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.utils.plot_model(model, show_shapes=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>




<p><img src="/Images/0407_Convolution_Neural_Network_02/output_12_0.png" alt="png"></p>
<pre><code>- 지금까지 한 것은 모델 정의
</code></pre>
<ul>
<li>모델 컴파일 후, 훈련</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># import tensorflow as tf</span></span><br><span class="line"></span><br><span class="line">model.<span class="built_in">compile</span>(optimizer=<span class="string">&#x27;adam&#x27;</span>, loss=<span class="string">&#x27;sparse_categorical_crossentropy&#x27;</span>, </span><br><span class="line">              metrics=<span class="string">&#x27;accuracy&#x27;</span>)</span><br><span class="line"></span><br><span class="line">checkpoint_cb = keras.callbacks.ModelCheckpoint(<span class="string">&#x27;best-cnn-model.h5&#x27;</span>, </span><br><span class="line">                                                save_best_only=<span class="literal">True</span>)</span><br><span class="line">early_stopping_cb = keras.callbacks.EarlyStopping(patience=<span class="number">2</span>,</span><br><span class="line">                                                  restore_best_weights=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># with tf.device(&#x27;/device:GPU:0&#x27;):</span></span><br><span class="line"></span><br><span class="line">history = model.fit(train_scaled, train_target, epochs=<span class="number">10</span>,</span><br><span class="line">                    validation_data=(val_scaled, val_target),</span><br><span class="line">                    callbacks=[checkpoint_cb, early_stopping_cb])</span><br></pre></td></tr></table></figure>

<pre><code>Epoch 1/10
1500/1500 [==============================] - 22s 7ms/step - loss: 0.5226 - accuracy: 0.8148 - val_loss: 0.3283 - val_accuracy: 0.8823
Epoch 2/10
1500/1500 [==============================] - 12s 8ms/step - loss: 0.3447 - accuracy: 0.8770 - val_loss: 0.2821 - val_accuracy: 0.8950
Epoch 3/10
1500/1500 [==============================] - 11s 7ms/step - loss: 0.2924 - accuracy: 0.8943 - val_loss: 0.2594 - val_accuracy: 0.8985
Epoch 4/10
1500/1500 [==============================] - 13s 8ms/step - loss: 0.2618 - accuracy: 0.9046 - val_loss: 0.2387 - val_accuracy: 0.9109
Epoch 5/10
1500/1500 [==============================] - 12s 8ms/step - loss: 0.2392 - accuracy: 0.9132 - val_loss: 0.2278 - val_accuracy: 0.9163
Epoch 6/10
1500/1500 [==============================] - 10s 7ms/step - loss: 0.2190 - accuracy: 0.9192 - val_loss: 0.2297 - val_accuracy: 0.9172
Epoch 7/10
1500/1500 [==============================] - 10s 7ms/step - loss: 0.2006 - accuracy: 0.9255 - val_loss: 0.2286 - val_accuracy: 0.9179
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># import tensorflow as tf</span></span><br><span class="line"><span class="comment"># model.compile(optimizer=&#x27;adam&#x27;, loss=&#x27;sparse_categorical_crossentropy&#x27;, </span></span><br><span class="line"><span class="comment">#               metrics=&#x27;accuracy&#x27;)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># checkpoint_cb = keras.callbacks.ModelCheckpoint(&#x27;best-cnn-model.h5&#x27;, </span></span><br><span class="line"><span class="comment">#                                                 save_best_only=True)</span></span><br><span class="line"><span class="comment"># early_stopping_cb = keras.callbacks.EarlyStopping(patience=2,</span></span><br><span class="line"><span class="comment">#                                                   restore_best_weights=True)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># with tf.device(&#x27;/device:GPU:0&#x27;):</span></span><br><span class="line"><span class="comment">#   history = model.fit(train_scaled, train_target, epochs=10,</span></span><br><span class="line"><span class="comment">#                       validation_data=(val_scaled, val_target),</span></span><br><span class="line"><span class="comment">#                       callbacks=[checkpoint_cb, early_stopping_cb])</span></span><br></pre></td></tr></table></figure>

<ul>
<li>모델 학습 곡선 그리기</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># # 연습 코드: plotly로 그려보기</span></span><br><span class="line"><span class="comment"># # 아래와 비슷한 결과를 얻었지만 블로그에 올리기 위해 주석 처리</span></span><br><span class="line"><span class="comment"># import plotly.express as px</span></span><br><span class="line"><span class="comment"># from plotly.subplots import make_subplots</span></span><br><span class="line"><span class="comment"># import plotly.graph_objects as go</span></span><br><span class="line"><span class="comment"># import matplotlib.pyplot as plt</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># fig = make_subplots()</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># fig.add_trace(go.Bar(y= history.history[&#x27;loss&#x27;]))</span></span><br><span class="line"><span class="comment"># fig.add_trace(go.Scatter(y= history.history[&#x27;val_loss&#x27;]))</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># fig.show()</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.plot(history.history[<span class="string">&#x27;loss&#x27;</span>])</span><br><span class="line">plt.plot(history.history[<span class="string">&#x27;val_loss&#x27;</span>])</span><br><span class="line">plt.xlabel(<span class="string">&#x27;epoch&#x27;</span>)</span><br><span class="line">plt.legend([<span class="string">&#x27;train&#x27;</span>, <span class="string">&#x27;val&#x27;</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="/Images/0407_Convolution_Neural_Network_02/output_19_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 그래픽 카드로 돌리는지 확인하는 방법</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">device_name = tf.test.gpu_device_name()</span><br><span class="line"><span class="keyword">if</span> device_name != <span class="string">&#x27;/device:GPU:0&#x27;</span>:</span><br><span class="line">  <span class="keyword">raise</span> SystemError(<span class="string">&#x27;GPU device not found&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Found GPU at: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(device_name))</span><br></pre></td></tr></table></figure>

<pre><code>Found GPU at: /device:GPU:0
</code></pre>
<h1 id="합성곱-신경망-시각화"><a href="#합성곱-신경망-시각화" class="headerlink" title="합성곱 신경망 시각화"></a>합성곱 신경망 시각화</h1><ul>
<li>model이 어떤 가중치를 학습했는지 확인</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br><span class="line">model = keras.models.load_model(<span class="string">&#x27;best-cnn-model.h5&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># keras.utils.plot_model(model2, show_shapes=True)</span></span><br><span class="line"></span><br><span class="line">model.layers  <span class="comment"># 리스트 형태로 저장되어 있음</span></span><br></pre></td></tr></table></figure>




<pre><code>[&lt;keras.layers.convolutional.Conv2D at 0x7fc7d81a7790&gt;,
 &lt;keras.layers.pooling.MaxPooling2D at 0x7fc8e00bfc10&gt;,
 &lt;keras.layers.convolutional.Conv2D at 0x7fc7f2f88490&gt;,
 &lt;keras.layers.pooling.MaxPooling2D at 0x7fc7d710d5d0&gt;,
 &lt;keras.layers.core.flatten.Flatten at 0x7fc8e023bb10&gt;,
 &lt;keras.layers.core.dense.Dense at 0x7fc7f3fb5350&gt;,
 &lt;keras.layers.core.dropout.Dropout at 0x7fc86a7a7050&gt;,
 &lt;keras.layers.core.dense.Dense at 0x7fc86a793750&gt;]



- 2번의 합성곱과 풀링을 하고 Flatten층, Dense층, Dropout층, Dense층이 보임
</code></pre>
<ul>
<li>합성곱층의 가중치를 확인 가능</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conv = model.layers[<span class="number">0</span>]</span><br><span class="line"><span class="built_in">print</span>(conv.weights[<span class="number">0</span>].shape, conv.weights[<span class="number">1</span>].shape)</span><br></pre></td></tr></table></figure>

<pre><code>(3, 3, 1, 32) (32,)


- 깊이가 1이므로 실제 커널 크기: (3, 3, 1)
- 필터 개수가 32개이므로 weights의 첫번째 원소인 가중치의 크기: (3, 3, 1, 32)
- weights의 두번째 원소는 절편의 개수를 나타냄
- 필터마다 1개의 절편이 있으므로 (32,)크기가 됨.
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conv_weights = conv.weights[<span class="number">0</span>].numpy()</span><br><span class="line"><span class="built_in">print</span>(conv_weights.mean(), conv_weights.std())  <span class="comment"># 평균(mean), 표준편차(std)</span></span><br></pre></td></tr></table></figure>

<pre><code>-0.025856383 0.21591602
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">plt.hist(conv_weights.reshape(-<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">plt.xlabel(<span class="string">&#x27;weight&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;count&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="/Images/0407_Convolution_Neural_Network_02/output_29_0.png" alt="png"></p>
<pre><code>- 0을 중심으로 종 모양 분포를 띄고 있음
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">fig, axs = plt.subplots(<span class="number">2</span>, <span class="number">16</span>, figsize=(<span class="number">15</span>,<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">16</span>):</span><br><span class="line">        axs[i, j].imshow(conv_weights[:,:,<span class="number">0</span>,i*<span class="number">16</span> + j], vmin=-<span class="number">0.5</span>, vmax=<span class="number">0.5</span>)</span><br><span class="line">        axs[i, j].axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="/Images/0407_Convolution_Neural_Network_02/output_31_0.png" alt="png"></p>
<ul>
<li>훈련하지 않은 빈 합성곱 신경망을 작성</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">no_training_model = keras.Sequential()</span><br><span class="line"></span><br><span class="line">no_training_model.add(keras.layers.Conv2D(<span class="number">32</span>, kernel_size=<span class="number">3</span>, activation=<span class="string">&#x27;relu&#x27;</span>, </span><br><span class="line">                                          padding=<span class="string">&#x27;same&#x27;</span>, input_shape=(<span class="number">28</span>,<span class="number">28</span>,<span class="number">1</span>)))</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">no_training_conv = no_training_model.layers[<span class="number">0</span>]</span><br><span class="line"><span class="built_in">print</span>(no_training_conv.weights[<span class="number">0</span>].shape)</span><br></pre></td></tr></table></figure>

<pre><code>(3, 3, 1, 32)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">no_training_weights = no_training_conv.weights[<span class="number">0</span>].numpy()</span><br><span class="line"><span class="built_in">print</span>(no_training_weights.mean(), no_training_weights.std())</span><br></pre></td></tr></table></figure>

<pre><code>0.0065383185 0.081879705
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">plt.hist(no_training_weights.reshape(-<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">plt.xlabel(<span class="string">&#x27;weight&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;count&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="/Images/0407_Convolution_Neural_Network_02/output_36_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">fig, axs = plt.subplots(<span class="number">2</span>, <span class="number">16</span>, figsize=(<span class="number">15</span>,<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">16</span>):</span><br><span class="line">        axs[i, j].imshow(no_training_weights[:,:,<span class="number">0</span>,i*<span class="number">16</span> + j], vmin=-<span class="number">0.5</span>, vmax=<span class="number">0.5</span>)</span><br><span class="line">        axs[i, j].axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="/Images/0407_Convolution_Neural_Network_02/output_37_0.png" alt="png"></p>
<pre><code>- 히스토그램에서 보앗듯이 전체적으로 가중치가 밋밋하게 초기화됨.
- 특징이 뚜렷하지 않음.
</code></pre>
<h1 id="함수형-API"><a href="#함수형-API" class="headerlink" title="함수형 API"></a>함수형 API</h1><ul>
<li>딥러닝에는 더 복잡한 모델이 많이 있음.<ul>
<li>입력이 2개일수도 있고 출력이 2개일수도 있는데 이런 경우엔 함수형API를 사용</li>
</ul>
</li>
</ul>
<h1 id="특성-맵-시각화"><a href="#특성-맵-시각화" class="headerlink" title="특성 맵 시각화"></a>특성 맵 시각화</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(model.<span class="built_in">input</span>)</span><br><span class="line">conv_acti = keras.Model(model.<span class="built_in">input</span>, model.layers[<span class="number">0</span>].output)</span><br><span class="line">(train_input, train_target), (test_input, test_target) = keras.datasets.fashion_mnist.load_data()</span><br><span class="line">plt.imshow(train_input[<span class="number">0</span>], cmap=<span class="string">&#x27;gray_r&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<pre><code>KerasTensor(type_spec=TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name=&#39;conv2d_2_input&#39;), name=&#39;conv2d_2_input&#39;, description=&quot;created by layer &#39;conv2d_2_input&#39;&quot;)
</code></pre>
<p><img src="/Images/0407_Convolution_Neural_Network_02/output_41_1.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">inputs = train_input[<span class="number">0</span>:<span class="number">1</span>].reshape(-<span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>)/<span class="number">255.0</span></span><br><span class="line">feature_maps = conv_acti.predict(inputs)</span><br><span class="line"><span class="built_in">print</span>(feature_maps.shape)</span><br></pre></td></tr></table></figure>

<pre><code>(1, 28, 28, 32)
</code></pre>
<ul>
<li>32개의 필터로 인해 입력 이미지에서 강하게 활성화 된 부분을 보여줌.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">fig, axs = plt.subplots(<span class="number">4</span>, <span class="number">8</span>, figsize=(<span class="number">15</span>,<span class="number">8</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">8</span>):</span><br><span class="line">        axs[i, j].imshow(feature_maps[<span class="number">0</span>,:,:,i*<span class="number">8</span> + j])</span><br><span class="line">        axs[i, j].axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="/Images/0407_Convolution_Neural_Network_02/output_44_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">conv2_acti = keras.Model(model.<span class="built_in">input</span>, model.layers[<span class="number">2</span>].output)</span><br><span class="line">feature_maps = conv2_acti.predict(train_input[<span class="number">0</span>:<span class="number">1</span>].reshape(-<span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>)/<span class="number">255.0</span>)</span><br><span class="line"><span class="built_in">print</span>(feature_maps.shape)</span><br><span class="line"></span><br><span class="line">fig, axs = plt.subplots(<span class="number">8</span>, <span class="number">8</span>, figsize=(<span class="number">12</span>,<span class="number">12</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">8</span>):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">8</span>):</span><br><span class="line">        axs[i, j].imshow(feature_maps[<span class="number">0</span>,:,:,i*<span class="number">8</span> + j])</span><br><span class="line">        axs[i, j].axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<pre><code>(1, 14, 14, 64)
</code></pre>
<p><img src="/Images/0407_Convolution_Neural_Network_02/output_45_1.png" alt="png"></p>
<ul>
<li><ul>
<li>합성곱 신경망의 앞부분에 있는 합성곱 층은 이미지의 시각적인 정보를 감지하고 뒤쪽에 있는 합성곱 층은 앞쪽에서 감지한 시각적인 정보를 바탕으로 추상적인 정보를 학습한다는 추론이 가능.</li>
</ul>
</li>
</ul>
<h1 id="마무리"><a href="#마무리" class="headerlink" title="마무리"></a>마무리</h1><ul>
<li><p>키워드</p>
<ul>
<li><strong>가중치 시각화</strong>: 합성곱 층의 가중치를 이미지로 출력하는것</li>
<li><strong>특성 맵 시각화</strong>: 합성곱 층의 활성화 출력을 이미지로 그리는 것</li>
<li><strong>함수형 API</strong>: 케라스에서 신경망 모델을 만드는 방법 중 하나. 전형적으로 입력은 Input()함수를 사용하고, 출력은 마지막 층의 출력으로 정의</li>
</ul>
</li>
<li><p><strong>TensolFlow</strong> 패키지</p>
<ul>
<li><strong>Conv2D</strong>: 입력의 너비와 높이 방향의 합성곱 연산을 구현한 클래스</li>
<li><strong>MaxPooling2D</strong>: 입력의 너비와 높이를 줄이는 풀링 연산을 구현한 클래스</li>
<li><strong>plot_model()</strong>: 케라스 모델 구조를 주피터 노트북에 그리거나 파일로 저장.</li>
<li><strong>Model</strong>: 케라스 모델을 만드는 클래스</li>
</ul>
</li>
<li><p><strong>matplotlib</strong> 패키지</p>
<ul>
<li><strong>bar()</strong>: 막대그래프를 출력</li>
</ul>
</li>
</ul>
</div></article></div><nav class="pagination" role="navigation" aria-label="pagination"><div class="pagination-previous"><a href="/">Previous</a></div><div class="pagination-next"><a href="/page/3/">Next</a></div><ul class="pagination-list is-hidden-mobile"><li><a class="pagination-link" href="/">1</a></li><li><a class="pagination-link is-current" href="/page/2/">2</a></li><li><a class="pagination-link" href="/page/3/">3</a></li><li><span class="pagination-ellipsis">&hellip;</span></li><li><a class="pagination-link" href="/page/6/">6</a></li></ul></nav></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/img/avatar.png" alt="Your name"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Your name</p><p class="is-size-6 is-block">Your title</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Your location</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">54</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">0</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">0</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/ppoffice" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/ppoffice"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Dribbble" href="https://dribbble.com"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><!--!--><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">Links</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://hexo.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Hexo</span></span><span class="level-right"><span class="level-item tag">hexo.io</span></span></a></li><li><a class="level is-mobile" href="https://bulma.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Bulma</span></span><span class="level-right"><span class="level-item tag">bulma.io</span></span></a></li></ul></div></div></div><!--!--><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-04-24T01:52:16.000Z">2022-04-24</time></p><p class="title"><a href="/2022/04/24/0424_Sellenium_LOLchampions_name/"> </a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-04-22T00:00:00.000Z">2022-04-22</time></p><p class="title"><a href="/2022/04/22/0422_Crawling_Selenium_on_gitbash/">Selenium 연습</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-04-20T00:00:00.000Z">2022-04-20</time></p><p class="title"><a href="/2022/04/20/0420_Apache_Spark_install_on_WSL/">PySpark 설치 WSL환경</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-04-20T00:00:00.000Z">2022-04-20</time></p><p class="title"><a href="/2022/04/20/0420_Pyspark_tutorial/">Pyspark 연습</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-04-19T00:00:00.000Z">2022-04-19</time></p><p class="title"><a href="/2022/04/19/0419_Apache_Spark_install/">Apache Spark 설치</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2022/04/"><span class="level-start"><span class="level-item">April 2022</span></span><span class="level-end"><span class="level-item tag">26</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/03/"><span class="level-start"><span class="level-item">March 2022</span></span><span class="level-end"><span class="level-item tag">28</span></span></a></li></ul></div></div></div><!--!--><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">Subscribe for updates</h3><form action="https://feedburner.google.com/fb/a/mailverify" method="post" target="popupwindow" onsubmit="window.open(&#039;https://feedburner.google.com/fb/a/mailverify?uri=&#039;,&#039;popupwindow&#039;,&#039;scrollbars=yes,width=550,height=520&#039;);return true"><input type="hidden" value="" name="uri"><input type="hidden" name="loc" value="en_US"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="Subscribe"></div></div></form></div></div></div><div class="card widget"><div class="card-content"><div class="notification is-danger">You need to set <code>client_id</code> and <code>slot_id</code> to show this AD unit. Please set it in <code>_config.yml</code>.</div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">follow.it</h3><form action="" method="post" target="_blank"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="Subscribe"></div></div></form></div></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.svg" alt="WB&#039;blog" height="28"></a><p class="is-size-7"><span>&copy; 2022 John Doe</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>