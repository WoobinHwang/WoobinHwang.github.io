<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>IMDB리뷰 - WB&#039;blog</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="WB&#039;blog"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="WB&#039;blog"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="순환 신경망으로 IMDB리뷰 분류 주제: 긍정리뷰, 부정리뷰 분류하기  텍스트 자체는 신경망에 전달하지 않음. (문자열 –&amp;gt; 수식에 적용 X) 문자열을 수식으로 정하는 규칙이 매우 가변적임. if ex) He follows the cat. He loves the cat.  10- 11——- 12- 13– 10- 14—- 12- 13  고양이를 따라간다"><meta property="og:type" content="blog"><meta property="og:title" content="IMDB리뷰"><meta property="og:url" content="https://woobinhwang.github.io/2022/04/08/0408_IMDB/"><meta property="og:site_name" content="WB&#039;blog"><meta property="og:description" content="순환 신경망으로 IMDB리뷰 분류 주제: 긍정리뷰, 부정리뷰 분류하기  텍스트 자체는 신경망에 전달하지 않음. (문자열 –&amp;gt; 수식에 적용 X) 문자열을 수식으로 정하는 규칙이 매우 가변적임. if ex) He follows the cat. He loves the cat.  10- 11——- 12- 13– 10- 14—- 12- 13  고양이를 따라간다"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://woobinhwang.github.io/Images/0408_IMDB/output_17_0.png"><meta property="og:image" content="https://woobinhwang.github.io/Images/0408_IMDB/output_37_0.png"><meta property="og:image" content="https://woobinhwang.github.io/Images/0408_IMDB/output_42_0.png"><meta property="article:published_time" content="2022-04-08T00:00:00.000Z"><meta property="article:modified_time" content="2022-04-08T08:34:23.721Z"><meta property="article:author" content="woobin"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/Images/0408_IMDB/output_17_0.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://woobinhwang.github.io/2022/04/08/0408_IMDB/"},"headline":"IMDB리뷰","image":["https://woobinhwang.github.io/Images/0408_IMDB/output_17_0.png","https://woobinhwang.github.io/Images/0408_IMDB/output_37_0.png","https://woobinhwang.github.io/Images/0408_IMDB/output_42_0.png"],"datePublished":"2022-04-08T00:00:00.000Z","dateModified":"2022-04-08T08:34:23.721Z","author":{"@type":"Person","name":"woobin"},"publisher":{"@type":"Organization","name":"WB'blog","logo":{"@type":"ImageObject","url":"https://woobinhwang.github.io/img/logo.svg"}},"description":"순환 신경망으로 IMDB리뷰 분류 주제: 긍정리뷰, 부정리뷰 분류하기  텍스트 자체는 신경망에 전달하지 않음. (문자열 –&gt; 수식에 적용 X) 문자열을 수식으로 정하는 규칙이 매우 가변적임. if ex) He follows the cat. He loves the cat.  10- 11——- 12- 13– 10- 14—- 12- 13  고양이를 따라간다"}</script><link rel="canonical" href="https://woobinhwang.github.io/2022/04/08/0408_IMDB/"><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.svg" alt="WB&#039;blog" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2022-04-08T00:00:00.000Z" title="2022. 4. 8. 오전 9:00:00">2022-04-08</time></span><span class="level-item">Updated&nbsp;<time dateTime="2022-04-08T08:34:23.721Z" title="2022. 4. 8. 오후 5:34:23">2022-04-08</time></span><span class="level-item"> woobin </span><span class="level-item">14 minutes read (About 2079 words)</span></div></div><h1 class="title is-3 is-size-4-mobile">IMDB리뷰</h1><div class="content"><h1 id="순환-신경망으로-IMDB리뷰-분류"><a href="#순환-신경망으로-IMDB리뷰-분류" class="headerlink" title="순환 신경망으로 IMDB리뷰 분류"></a>순환 신경망으로 IMDB리뷰 분류</h1><ul>
<li><p>주제: 긍정리뷰, 부정리뷰 분류하기</p>
<ul>
<li>텍스트 자체는 신경망에 전달하지 않음. (문자열 –&gt; 수식에 적용 X)</li>
<li>문자열을 수식으로 정하는 규칙이 매우 가변적임.</li>
<li>if ex)<ul>
<li><p>He follows the cat. He loves the cat.</p>
</li>
<li><p>10- 11——- 12- 13– 10- 14—- 12- 13</p>
</li>
<li><p>고양이를 따라간다. He follows the cat.</p>
</li>
<li><p>10———– 11———- 12– 13——- 14- 15</p>
</li>
</ul>
</li>
</ul>
</li>
<li><p>RNN, LSTM 알고리즘</p>
<ul>
<li>영어권 사람들이 만듬</li>
<li>자연어 처리와 관련된 많은 알고리즘 또한 영어권 사람들이 만듬</li>
</ul>
</li>
<li><p>한글 !&#x3D; 영어 ( 언어가 다름을 인지 )</p>
<ul>
<li>성과 내려면 제품을 써야함 (&#x3D;돈) (네이버 &#x2F; 카카오쪽 제품)</li>
</ul>
</li>
</ul>
<h1 id="데이터-불러오기"><a href="#데이터-불러오기" class="headerlink" title="데이터 불러오기"></a>데이터 불러오기</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras.datasets <span class="keyword">import</span> imdb</span><br><span class="line">(train_input, train_target), (test_input, test_target) = imdb.load_data(num_words= <span class="number">500</span>)</span><br></pre></td></tr></table></figure>

<pre><code>Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz
17465344/17464789 [==============================] - 0s 0us/step
17473536/17464789 [==============================] - 0s 0us/step
</code></pre>
<ul>
<li>데이터 크기 확인 (1차원 배열)</li>
<li>텍스트의 길이가 다 다르기 때문에 1차원 배열로 정리되어 있음</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(train_input.shape, test_input.shape)</span><br></pre></td></tr></table></figure>

<pre><code>(25000,) (25000,)
</code></pre>
<ul>
<li>문장의 길이 확인<ul>
<li>문장의 길이가 다 다름</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(train_input[<span class="number">0</span>]))</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(train_input[<span class="number">1</span>]))</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(train_input[<span class="number">2</span>]))</span><br></pre></td></tr></table></figure>

<pre><code>218
189
141
</code></pre>
<ul>
<li>Raw 데이터 전처리 -&gt; 토큰화 작업이 끝난 상황 (문자열이 숫자로 바뀜)</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(train_input[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>

<pre><code>[1, 14, 22, 16, 43, 2, 2, 2, 2, 65, 458, 2, 66, 2, 4, 173, 36, 256, 5, 25, 100, 43, 2, 112, 50, 2, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 2, 2, 17, 2, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2, 19, 14, 22, 4, 2, 2, 469, 4, 22, 71, 87, 12, 16, 43, 2, 38, 76, 15, 13, 2, 4, 22, 17, 2, 17, 12, 16, 2, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2, 2, 16, 480, 66, 2, 33, 4, 130, 12, 16, 38, 2, 5, 25, 124, 51, 36, 135, 48, 25, 2, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 2, 5, 2, 36, 71, 43, 2, 476, 26, 400, 317, 46, 7, 4, 2, 2, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 2, 88, 12, 16, 283, 5, 16, 2, 113, 103, 32, 15, 16, 2, 19, 178, 32]
</code></pre>
<ul>
<li>Target 데이터 출력<ul>
<li>0은 부정 리뷰</li>
<li>1은 긍정 리뷰</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 0번째에서 19번째까지의 데이터</span></span><br><span class="line"><span class="built_in">print</span>(train_target[:<span class="number">20</span>])</span><br></pre></td></tr></table></figure>

<pre><code>[1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 0 1]
</code></pre>
<h1 id="데이터셋-분리"><a href="#데이터셋-분리" class="headerlink" title="데이터셋 분리"></a>데이터셋 분리</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">train_input, val_input, train_target, val_target = train_test_split(</span><br><span class="line">    train_input, train_target, test_size= <span class="number">0.2</span>, random_state= <span class="number">42</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_input.shape, train_target.shape, val_input.shape, val_target.shape</span><br></pre></td></tr></table></figure>




<pre><code>((20000,), (20000,), (5000,), (5000,))
</code></pre>
<h1 id="데이터-시각화"><a href="#데이터-시각화" class="headerlink" title="데이터 시각화"></a>데이터 시각화</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment">#  컴프리헨션으로 길이를 표현하는 배열 만들기</span></span><br><span class="line">lengths= np.array([<span class="built_in">len</span>(x) <span class="keyword">for</span> x <span class="keyword">in</span> train_input])</span><br><span class="line"><span class="built_in">print</span>(np.mean(lengths), np.median(lengths), np.<span class="built_in">max</span>(lengths))</span><br></pre></td></tr></table></figure>

<pre><code>238.71364 178.0 2494
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.hist(lengths)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="/Images/0408_IMDB/output_17_0.png" alt="png"></p>
<pre><code>- 250이하의 길이가 대부분임을 알 수 있음.
</code></pre>
<ul>
<li>짧은 단어 100개만 사용 할 예정</li>
<li>모든 길이를 100에 맞추는 ‘패딩’작업 실행</li>
<li>데이터의 갯수는 20000, 전체 길이는 100으로 맞춤</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras.preprocessing.sequence <span class="keyword">import</span> pad_sequences</span><br><span class="line">train_seq= pad_sequences(train_input, maxlen= <span class="number">100</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(train_seq.shape)</span><br></pre></td></tr></table></figure>

<pre><code>(25000, 100)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">len</span>(train_input[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>




<pre><code>218
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(train_seq[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>

<pre><code>[  2  33   6  22  12 215  28  77  52   5  14 407  16  82   2   8   4 107
 117   2  15 256   4   2   7   2   5   2  36  71  43   2 476  26 400 317
  46   7   4   2   2  13 104  88   4 381  15 297  98  32   2  56  26 141
   6 194   2  18   4 226  22  21 134 476  26 480   5 144  30   2  18  51
  36  28 224  92  25 104   4 226  65  16  38   2  88  12  16 283   5  16
   2 113 103  32  15  16   2  19 178  32]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(train_input[<span class="number">0</span>][-<span class="number">10</span>:])  <span class="comment"># 음수의 인덱스로 슬라이싱 하면 끝에서부터 세기 시작함.</span></span><br><span class="line"><span class="built_in">print</span>()</span><br><span class="line"><span class="built_in">print</span>(train_input[<span class="number">0</span>][<span class="number">208</span>:])</span><br></pre></td></tr></table></figure>

<pre><code>[2, 113, 103, 32, 15, 16, 2, 19, 178, 32]

[2, 113, 103, 32, 15, 16, 2, 19, 178, 32]


- train_seq[0]의 길이는 218로 100보다 길기때문에 패딩이 안되었음을 확인 할 수 있음
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val_seq = pad_sequences(val_input, maxlen= <span class="number">100</span>)</span><br><span class="line">val_seq</span><br></pre></td></tr></table></figure>




<pre><code>array([[ 32,   2, 225, ...,  14,  58,   2],
       [ 53,   2,   8, ...,   7,  32,   2],
       [  0,   0,   0, ...,   2,  33,  32],
       ...,
       [383,   2, 120, ...,  16,  99,  76],
       [106, 345,  12, ..., 120,   2, 156],
       [  4, 114,  21, ...,   4,   2,   2]], dtype=int32)
</code></pre>
<h1 id="순환신경망-만들기"><a href="#순환신경망-만들기" class="headerlink" title="순환신경망 만들기"></a>순환신경망 만들기</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br><span class="line">model = keras.Sequential()</span><br><span class="line"><span class="comment"># 최대 길이는 100까지 패딩, imdb.load_data() 함수에서 500개의 단어만 사용하도록 지정했었음</span></span><br><span class="line">model.add(keras.layers.SimpleRNN(<span class="number">8</span>, input_shape= (<span class="number">100</span>, <span class="number">500</span>)))</span><br><span class="line">model.add(keras.layers.Dense(<span class="number">1</span>, activation= <span class="string">&#x27;sigmoid&#x27;</span>))</span><br></pre></td></tr></table></figure>

<ul>
<li>원핫인코딩 적용</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># to_categorical(): 원핫인코딩된 배열로 반환해줌.</span></span><br><span class="line">train_oh = keras.utils.to_categorical(train_seq)</span><br><span class="line"><span class="built_in">print</span>(train_oh.shape)</span><br></pre></td></tr></table></figure>

<pre><code>(20000, 100, 500)


- 정수 하나마다 500차원의 (20000, 100)의 크기로 들어가있음을 확인 할 수 있음
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(train_oh[<span class="number">0</span>][<span class="number">0</span>][:<span class="number">12</span>])</span><br></pre></td></tr></table></figure>

<pre><code>[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 나머지 원소에 전부 0으로 들어가있는지 확인</span></span><br><span class="line"><span class="built_in">print</span>(np.<span class="built_in">sum</span>(train_oh[<span class="number">0</span>][<span class="number">0</span>]))</span><br></pre></td></tr></table></figure>

<pre><code>1.0
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val_oh = keras.utils.to_categorical(val_seq)</span><br><span class="line"><span class="built_in">print</span>(val_oh.shape)</span><br></pre></td></tr></table></figure>

<pre><code>(5000, 100, 500)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.summary()</span><br></pre></td></tr></table></figure>

<pre><code>Model: &quot;sequential_3&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 simple_rnn_3 (SimpleRNN)    (None, 8)                 4072      
                                                                 
 dense_3 (Dense)             (None, 1)                 9         
                                                                 
=================================================================
Total params: 4,081
Trainable params: 4,081
Non-trainable params: 0
_________________________________________________________________
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">rmsprop = keras.optimizers.RMSprop(learning_rate=<span class="number">1e-4</span>)</span><br><span class="line">model.<span class="built_in">compile</span>(optimizer=rmsprop, loss=<span class="string">&#x27;binary_crossentropy&#x27;</span>, </span><br><span class="line">              metrics=[<span class="string">&#x27;accuracy&#x27;</span>])</span><br><span class="line"></span><br><span class="line">checkpoint_cb = keras.callbacks.ModelCheckpoint(<span class="string">&#x27;best-simplernn-model.h5&#x27;</span>, </span><br><span class="line">                                                save_best_only=<span class="literal">True</span>)</span><br><span class="line">early_stopping_cb = keras.callbacks.EarlyStopping(patience=<span class="number">3</span>,</span><br><span class="line">                                                  restore_best_weights=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 원본 epochs = 100</span></span><br><span class="line">history = model.fit(train_oh, train_target, epochs=<span class="number">100</span>, batch_size=<span class="number">64</span>,</span><br><span class="line">                    validation_data=(val_oh, val_target),</span><br><span class="line">                    callbacks=[checkpoint_cb, early_stopping_cb])</span><br></pre></td></tr></table></figure>

<pre><code>Epoch 1/100
313/313 [==============================] - 15s 44ms/step - loss: 0.5339 - accuracy: 0.7710 - val_loss: 0.5444 - val_accuracy: 0.7566
Epoch 2/100
313/313 [==============================] - 11s 36ms/step - loss: 0.5241 - accuracy: 0.7762 - val_loss: 0.5373 - val_accuracy: 0.7528
Epoch 3/100
313/313 [==============================] - 12s 37ms/step - loss: 0.5160 - accuracy: 0.7796 - val_loss: 0.5296 - val_accuracy: 0.7576
Epoch 4/100
313/313 [==============================] - 11s 36ms/step - loss: 0.5079 - accuracy: 0.7828 - val_loss: 0.5231 - val_accuracy: 0.7592
Epoch 5/100
313/313 [==============================] - 12s 37ms/step - loss: 0.4998 - accuracy: 0.7872 - val_loss: 0.5165 - val_accuracy: 0.7660
Epoch 6/100
313/313 [==============================] - 11s 36ms/step - loss: 0.4931 - accuracy: 0.7887 - val_loss: 0.5115 - val_accuracy: 0.7666
Epoch 7/100
313/313 [==============================] - 12s 37ms/step - loss: 0.4858 - accuracy: 0.7914 - val_loss: 0.5061 - val_accuracy: 0.7710
Epoch 8/100
313/313 [==============================] - 11s 36ms/step - loss: 0.4801 - accuracy: 0.7949 - val_loss: 0.5015 - val_accuracy: 0.7690
Epoch 9/100
313/313 [==============================] - 11s 37ms/step - loss: 0.4742 - accuracy: 0.7972 - val_loss: 0.4985 - val_accuracy: 0.7684
Epoch 10/100
313/313 [==============================] - 12s 38ms/step - loss: 0.4688 - accuracy: 0.7982 - val_loss: 0.4931 - val_accuracy: 0.7744
Epoch 11/100
313/313 [==============================] - 16s 50ms/step - loss: 0.4636 - accuracy: 0.8023 - val_loss: 0.4941 - val_accuracy: 0.7674
Epoch 12/100
313/313 [==============================] - 11s 36ms/step - loss: 0.4595 - accuracy: 0.8023 - val_loss: 0.4893 - val_accuracy: 0.7732
Epoch 13/100
313/313 [==============================] - 11s 36ms/step - loss: 0.4546 - accuracy: 0.8051 - val_loss: 0.4850 - val_accuracy: 0.7782
Epoch 14/100
313/313 [==============================] - 11s 36ms/step - loss: 0.4512 - accuracy: 0.8065 - val_loss: 0.4837 - val_accuracy: 0.7760
Epoch 15/100
313/313 [==============================] - 11s 36ms/step - loss: 0.4471 - accuracy: 0.8097 - val_loss: 0.4826 - val_accuracy: 0.7766
Epoch 16/100
313/313 [==============================] - 11s 36ms/step - loss: 0.4438 - accuracy: 0.8109 - val_loss: 0.4789 - val_accuracy: 0.7796
Epoch 17/100
313/313 [==============================] - 11s 36ms/step - loss: 0.4396 - accuracy: 0.8121 - val_loss: 0.4795 - val_accuracy: 0.7764
Epoch 18/100
313/313 [==============================] - 12s 38ms/step - loss: 0.4367 - accuracy: 0.8133 - val_loss: 0.4792 - val_accuracy: 0.7760
Epoch 19/100
313/313 [==============================] - 12s 40ms/step - loss: 0.4344 - accuracy: 0.8127 - val_loss: 0.4761 - val_accuracy: 0.7784
Epoch 20/100
313/313 [==============================] - 15s 47ms/step - loss: 0.4313 - accuracy: 0.8157 - val_loss: 0.4733 - val_accuracy: 0.7822
Epoch 21/100
313/313 [==============================] - 11s 37ms/step - loss: 0.4289 - accuracy: 0.8162 - val_loss: 0.4753 - val_accuracy: 0.7794
Epoch 22/100
313/313 [==============================] - 11s 35ms/step - loss: 0.4261 - accuracy: 0.8173 - val_loss: 0.4757 - val_accuracy: 0.7806
Epoch 23/100
313/313 [==============================] - 11s 36ms/step - loss: 0.4241 - accuracy: 0.8167 - val_loss: 0.4717 - val_accuracy: 0.7826
Epoch 24/100
313/313 [==============================] - 11s 36ms/step - loss: 0.4220 - accuracy: 0.8188 - val_loss: 0.4735 - val_accuracy: 0.7816
Epoch 25/100
313/313 [==============================] - 12s 37ms/step - loss: 0.4197 - accuracy: 0.8202 - val_loss: 0.4696 - val_accuracy: 0.7820
Epoch 26/100
313/313 [==============================] - 11s 36ms/step - loss: 0.4179 - accuracy: 0.8201 - val_loss: 0.4728 - val_accuracy: 0.7834
Epoch 27/100
313/313 [==============================] - 11s 36ms/step - loss: 0.4160 - accuracy: 0.8210 - val_loss: 0.4736 - val_accuracy: 0.7830
Epoch 28/100
313/313 [==============================] - 11s 36ms/step - loss: 0.4142 - accuracy: 0.8209 - val_loss: 0.4711 - val_accuracy: 0.7836
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(history.history[<span class="string">&#x27;loss&#x27;</span>])</span><br><span class="line">plt.plot(history.history[<span class="string">&#x27;val_loss&#x27;</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="/Images/0408_IMDB/output_37_0.png" alt="png"></p>
<h1 id="단어-임베딩을-사용"><a href="#단어-임베딩을-사용" class="headerlink" title="단어 임베딩을 사용"></a>단어 임베딩을 사용</h1><ul>
<li>문제점 발생: 토큰 1개를 500차원으로 늘림<ul>
<li>-&gt; 데이터 크기가 500배 커짐</li>
</ul>
</li>
<li>단어임베딩: 라벨인코딩과 비슷</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br><span class="line">model2 = keras.Sequential()</span><br><span class="line">model2.add(keras.layers.Embedding(<span class="number">500</span>, <span class="number">16</span>, input_length= <span class="number">100</span>))</span><br><span class="line">model2.add(keras.layers.SimpleRNN(<span class="number">8</span>))</span><br><span class="line">model2.add(keras.layers.Dense(<span class="number">1</span>, activation= <span class="string">&#x27;sigmoid&#x27;</span>))</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model2.summary()</span><br></pre></td></tr></table></figure>

<pre><code>Model: &quot;sequential_4&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 embedding_2 (Embedding)     (None, 100, 16)           8000      
                                                                 
 simple_rnn_4 (SimpleRNN)    (None, 8)                 200       
                                                                 
 dense_4 (Dense)             (None, 1)                 9         
                                                                 
=================================================================
Total params: 8,209
Trainable params: 8,209
Non-trainable params: 0
_________________________________________________________________
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">rmsprop = keras.optimizers.RMSprop(learning_rate=<span class="number">1e-4</span>)</span><br><span class="line">model2.<span class="built_in">compile</span>(optimizer=rmsprop, loss=<span class="string">&#x27;binary_crossentropy&#x27;</span>, </span><br><span class="line">              metrics=[<span class="string">&#x27;accuracy&#x27;</span>])</span><br><span class="line"><span class="comment"># 임베딩으로 Change</span></span><br><span class="line">checkpoint_cb = keras.callbacks.ModelCheckpoint(<span class="string">&#x27;best-embedding-model.h5&#x27;</span>, </span><br><span class="line">                                                save_best_only=<span class="literal">True</span>)</span><br><span class="line">early_stopping_cb = keras.callbacks.EarlyStopping(patience=<span class="number">3</span>,</span><br><span class="line">                                                  restore_best_weights=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 원본 epochs= 100</span></span><br><span class="line">history = model2.fit(train_seq, train_target, epochs=<span class="number">100</span>, batch_size=<span class="number">64</span>,</span><br><span class="line">                    validation_data=(val_seq, val_target),</span><br><span class="line">                    callbacks=[checkpoint_cb, early_stopping_cb])</span><br></pre></td></tr></table></figure>

<pre><code>Epoch 1/100
313/313 [==============================] - 8s 22ms/step - loss: 0.6812 - accuracy: 0.5584 - val_loss: 0.6570 - val_accuracy: 0.6208
Epoch 2/100
313/313 [==============================] - 6s 19ms/step - loss: 0.6334 - accuracy: 0.6675 - val_loss: 0.6146 - val_accuracy: 0.6916
Epoch 3/100
313/313 [==============================] - 6s 19ms/step - loss: 0.5940 - accuracy: 0.7219 - val_loss: 0.5831 - val_accuracy: 0.7314
Epoch 4/100
313/313 [==============================] - 6s 19ms/step - loss: 0.5655 - accuracy: 0.7492 - val_loss: 0.5596 - val_accuracy: 0.7450
Epoch 5/100
313/313 [==============================] - 7s 21ms/step - loss: 0.5455 - accuracy: 0.7593 - val_loss: 0.5461 - val_accuracy: 0.7518
Epoch 6/100
313/313 [==============================] - 6s 20ms/step - loss: 0.5302 - accuracy: 0.7678 - val_loss: 0.5405 - val_accuracy: 0.7446
Epoch 7/100
313/313 [==============================] - 6s 19ms/step - loss: 0.5170 - accuracy: 0.7740 - val_loss: 0.5327 - val_accuracy: 0.7492
Epoch 8/100
313/313 [==============================] - 7s 21ms/step - loss: 0.5076 - accuracy: 0.7765 - val_loss: 0.5195 - val_accuracy: 0.7598
Epoch 9/100
313/313 [==============================] - 6s 19ms/step - loss: 0.4996 - accuracy: 0.7814 - val_loss: 0.5141 - val_accuracy: 0.7592
Epoch 10/100
313/313 [==============================] - 6s 20ms/step - loss: 0.4908 - accuracy: 0.7845 - val_loss: 0.5085 - val_accuracy: 0.7642
Epoch 11/100
313/313 [==============================] - 6s 20ms/step - loss: 0.4844 - accuracy: 0.7876 - val_loss: 0.5082 - val_accuracy: 0.7598
Epoch 12/100
313/313 [==============================] - 6s 20ms/step - loss: 0.4782 - accuracy: 0.7904 - val_loss: 0.4958 - val_accuracy: 0.7722
Epoch 13/100
313/313 [==============================] - 6s 21ms/step - loss: 0.4711 - accuracy: 0.7937 - val_loss: 0.4989 - val_accuracy: 0.7644
Epoch 14/100
313/313 [==============================] - 6s 20ms/step - loss: 0.4640 - accuracy: 0.7980 - val_loss: 0.4876 - val_accuracy: 0.7740
Epoch 15/100
313/313 [==============================] - 6s 19ms/step - loss: 0.4607 - accuracy: 0.7991 - val_loss: 0.4833 - val_accuracy: 0.7770
Epoch 16/100
313/313 [==============================] - 6s 20ms/step - loss: 0.4541 - accuracy: 0.8021 - val_loss: 0.4896 - val_accuracy: 0.7682
Epoch 17/100
313/313 [==============================] - 6s 20ms/step - loss: 0.4496 - accuracy: 0.8054 - val_loss: 0.4753 - val_accuracy: 0.7810
Epoch 18/100
313/313 [==============================] - 6s 20ms/step - loss: 0.4458 - accuracy: 0.8079 - val_loss: 0.4748 - val_accuracy: 0.7790
Epoch 19/100
313/313 [==============================] - 6s 19ms/step - loss: 0.4421 - accuracy: 0.8102 - val_loss: 0.4753 - val_accuracy: 0.7786
Epoch 20/100
313/313 [==============================] - 6s 19ms/step - loss: 0.4390 - accuracy: 0.8127 - val_loss: 0.4717 - val_accuracy: 0.7772
Epoch 21/100
313/313 [==============================] - 6s 19ms/step - loss: 0.4351 - accuracy: 0.8131 - val_loss: 0.4823 - val_accuracy: 0.7732
Epoch 22/100
313/313 [==============================] - 6s 20ms/step - loss: 0.4331 - accuracy: 0.8137 - val_loss: 0.4659 - val_accuracy: 0.7842
Epoch 23/100
313/313 [==============================] - 7s 22ms/step - loss: 0.4301 - accuracy: 0.8145 - val_loss: 0.4655 - val_accuracy: 0.7812
Epoch 24/100
313/313 [==============================] - 6s 20ms/step - loss: 0.4260 - accuracy: 0.8174 - val_loss: 0.4771 - val_accuracy: 0.7754
Epoch 25/100
313/313 [==============================] - 6s 21ms/step - loss: 0.4243 - accuracy: 0.8185 - val_loss: 0.4637 - val_accuracy: 0.7846
Epoch 26/100
313/313 [==============================] - 6s 19ms/step - loss: 0.4208 - accuracy: 0.8205 - val_loss: 0.4658 - val_accuracy: 0.7834
Epoch 27/100
313/313 [==============================] - 6s 20ms/step - loss: 0.4185 - accuracy: 0.8222 - val_loss: 0.4652 - val_accuracy: 0.7820
Epoch 28/100
313/313 [==============================] - 6s 20ms/step - loss: 0.4161 - accuracy: 0.8236 - val_loss: 0.4609 - val_accuracy: 0.7866
Epoch 29/100
313/313 [==============================] - 7s 21ms/step - loss: 0.4136 - accuracy: 0.8249 - val_loss: 0.4642 - val_accuracy: 0.7860
Epoch 30/100
313/313 [==============================] - 6s 19ms/step - loss: 0.4113 - accuracy: 0.8254 - val_loss: 0.4601 - val_accuracy: 0.7876
Epoch 31/100
313/313 [==============================] - 6s 20ms/step - loss: 0.4090 - accuracy: 0.8281 - val_loss: 0.4643 - val_accuracy: 0.7824
Epoch 32/100
313/313 [==============================] - 6s 20ms/step - loss: 0.4076 - accuracy: 0.8286 - val_loss: 0.4622 - val_accuracy: 0.7874
Epoch 33/100
313/313 [==============================] - 6s 19ms/step - loss: 0.4053 - accuracy: 0.8289 - val_loss: 0.4590 - val_accuracy: 0.7882
Epoch 34/100
313/313 [==============================] - 6s 21ms/step - loss: 0.4030 - accuracy: 0.8299 - val_loss: 0.4572 - val_accuracy: 0.7902
Epoch 35/100
313/313 [==============================] - 7s 21ms/step - loss: 0.4010 - accuracy: 0.8321 - val_loss: 0.4632 - val_accuracy: 0.7878
Epoch 36/100
313/313 [==============================] - 6s 19ms/step - loss: 0.3995 - accuracy: 0.8331 - val_loss: 0.4571 - val_accuracy: 0.7914
Epoch 37/100
313/313 [==============================] - 6s 19ms/step - loss: 0.3973 - accuracy: 0.8347 - val_loss: 0.4621 - val_accuracy: 0.7892
Epoch 38/100
313/313 [==============================] - 7s 21ms/step - loss: 0.3958 - accuracy: 0.8353 - val_loss: 0.4568 - val_accuracy: 0.7916
Epoch 39/100
313/313 [==============================] - 6s 20ms/step - loss: 0.3941 - accuracy: 0.8354 - val_loss: 0.4626 - val_accuracy: 0.7868
Epoch 40/100
313/313 [==============================] - 7s 22ms/step - loss: 0.3927 - accuracy: 0.8358 - val_loss: 0.4611 - val_accuracy: 0.7886
Epoch 41/100
313/313 [==============================] - 6s 20ms/step - loss: 0.3907 - accuracy: 0.8371 - val_loss: 0.4583 - val_accuracy: 0.7912
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(history.history[<span class="string">&#x27;loss&#x27;</span>])</span><br><span class="line">plt.plot(history.history[<span class="string">&#x27;val_loss&#x27;</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="/Images/0408_IMDB/output_42_0.png" alt="png"></p>
<h1 id="마무리-정리"><a href="#마무리-정리" class="headerlink" title="마무리 정리"></a>마무리 정리</h1><ul>
<li><p>키워드</p>
<ul>
<li><strong>말뭉치</strong>: 자연어 처리에서 사용하는 텍스트 데이터의 모음. (&#x3D; 훈련 데이터셋)</li>
<li><strong>토큰</strong>: 텍스트에서 공백으로 구분되는 문자열 종종 소문자로 변환하고 구둣점은 삭제</li>
<li><strong>원-핫 인코딩</strong>: 어떤 클래스에 해당하는 원소만 1이고 나머지는 모두 0인 벡터</li>
<li><strong>단어 임베딩</strong>: 정수로 변환된 토큰을 비교적 작은 크기의 실수 밀집 벡터로 변환. 이런 밀집 벡터는 단어 사이의 관계를 표현 할 수 있기 때문에 자연어 처리에서 좋은 성능을 발휘</li>
</ul>
</li>
<li><p><strong>TensorFlow 패키지</strong></p>
<ul>
<li><strong>pad_sequences()</strong>: 시퀀스 길이를 맞추기 위해 패딩을 추가. (샘플 개수, 타임스텝 개수)크기의 2차원 배열로 나옴.<ul>
<li>매개변수 maxlen: 원하는 시퀀스 길이를 지정. 지정한 값보다 긴 시퀀스는 잘리고, 짧은 시퀀스는 패딩됨.</li>
</ul>
</li>
<li><strong>to_categorical()</strong>: 정수 시퀀스를 원 핫 인코딩으로 변환. 토큰이나 타깃값을 원핫 인코딩 할 때 사용</li>
<li><strong>SumpleRNN</strong>: 케라스의 기본 순환층 클래스</li>
<li><strong>Embedding</strong>: 단어 임베딩을 위한 클래스</li>
</ul>
</li>
</ul>
</div><div class="article-licensing box"><div class="licensing-title"><p>IMDB리뷰</p><p><a href="https://woobinhwang.github.io/2022/04/08/0408_IMDB/">https://woobinhwang.github.io/2022/04/08/0408_IMDB/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>Author</h6><p>woobin</p></div></div><div class="level-item is-narrow"><div><h6>Posted on</h6><p>2022-04-08</p></div></div><div class="level-item is-narrow"><div><h6>Updated on</h6><p>2022-04-08</p></div></div><div class="level-item is-narrow"><div><h6>Licensed under</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="notification is-danger">You need to set <code>install_url</code> to use ShareThis. Please set it in <code>_config.yml</code>.</div></article></div><div class="card"><div class="card-content"><h3 class="menu-label has-text-centered">Like this article? Support the author with</h3><div class="buttons is-centered"><a class="button donate" href="/" target="_blank" rel="noopener" data-type="afdian"><span class="icon is-small"><i class="fas fa-charging-station"></i></span><span>Afdian.net</span></a><a class="button donate" data-type="alipay"><span class="icon is-small"><i class="fab fa-alipay"></i></span><span>Alipay</span><span class="qrcode"><img src="/" alt="Alipay"></span></a><a class="button donate" href="/" target="_blank" rel="noopener" data-type="buymeacoffee"><span class="icon is-small"><i class="fas fa-coffee"></i></span><span>Buy me a coffee</span></a><a class="button donate" href="/" target="_blank" rel="noopener" data-type="patreon"><span class="icon is-small"><i class="fab fa-patreon"></i></span><span>Patreon</span></a><div class="notification is-danger">You forgot to set the <code>business</code> or <code>currency_code</code> for Paypal. Please set it in <code>_config.yml</code>.</div><a class="button donate" data-type="wechat"><span class="icon is-small"><i class="fab fa-weixin"></i></span><span>Wechat</span><span class="qrcode"><img src="/" alt="Wechat"></span></a></div></div></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/2022/04/08/0408_LSTM,GRU_Cell/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">LSTM과 GRU셀</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2022/04/07/0407_Sequentoal_Data,Recurrent_Neural_Network/"><span class="level-item">순차 데이터와 순환 신경망</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">Comments</h3><div class="notification is-danger">You forgot to set the <code>shortname</code> for Disqus. Please set it in <code>_config.yml</code>.</div></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/img/avatar.png" alt="Your name"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Your name</p><p class="is-size-6 is-block">Your title</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Your location</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">60</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">0</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">0</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/ppoffice" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/ppoffice"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Dribbble" href="https://dribbble.com"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><!--!--><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">Links</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://hexo.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Hexo</span></span><span class="level-right"><span class="level-item tag">hexo.io</span></span></a></li><li><a class="level is-mobile" href="https://bulma.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Bulma</span></span><span class="level-right"><span class="level-item tag">bulma.io</span></span></a></li></ul></div></div></div><!--!--><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-04-26T00:00:00.000Z">2022-04-26</time></p><p class="title"><a href="/2022/04/26/0426_SQL_Query/">SQL Query문</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-04-26T00:00:00.000Z">2022-04-26</time></p><p class="title"><a href="/2022/04/26/0426_Database_Object_02/">데이터베이스의 객체 2</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-04-25T00:00:00.000Z">2022-04-25</time></p><p class="title"><a href="/2022/04/25/0425_DataBase_Object/">데이터베이스의 객체</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-04-25T00:00:00.000Z">2022-04-25</time></p><p class="title"><a href="/2022/04/25/0425_Exception_Handling/">예외 처리</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-04-25T00:00:00.000Z">2022-04-25</time></p><p class="title"><a href="/2022/04/25/0425_Oracle_SQL_deinsert/">Oracle SQL 삭제</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2022/04/"><span class="level-start"><span class="level-item">April 2022</span></span><span class="level-end"><span class="level-item tag">32</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/03/"><span class="level-start"><span class="level-item">March 2022</span></span><span class="level-end"><span class="level-item tag">28</span></span></a></li></ul></div></div></div><!--!--><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">Subscribe for updates</h3><form action="https://feedburner.google.com/fb/a/mailverify" method="post" target="popupwindow" onsubmit="window.open(&#039;https://feedburner.google.com/fb/a/mailverify?uri=&#039;,&#039;popupwindow&#039;,&#039;scrollbars=yes,width=550,height=520&#039;);return true"><input type="hidden" value="" name="uri"><input type="hidden" name="loc" value="en_US"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="Subscribe"></div></div></form></div></div></div><div class="card widget"><div class="card-content"><div class="notification is-danger">You need to set <code>client_id</code> and <code>slot_id</code> to show this AD unit. Please set it in <code>_config.yml</code>.</div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">follow.it</h3><form action="" method="post" target="_blank"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="Subscribe"></div></div></form></div></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.svg" alt="WB&#039;blog" height="28"></a><p class="is-size-7"><span>&copy; 2022 woobin</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>