<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>심층 신경망 | WB&#39;blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="심증 신경망123from tensorflow import keras(train_input, train_target), (test_input, test_target) &#x3D; keras.datasets.fashion_mnist.load_data()   1234567from sklearn.model_selection import train_test_splittrai">
<meta property="og:type" content="article">
<meta property="og:title" content="심층 신경망">
<meta property="og:url" content="https://woobinhwang.github.io/2022/04/04/0404_Deep_Neural_Network/index.html">
<meta property="og:site_name" content="WB&#39;blog">
<meta property="og:description" content="심증 신경망123from tensorflow import keras(train_input, train_target), (test_input, test_target) &#x3D; keras.datasets.fashion_mnist.load_data()   1234567from sklearn.model_selection import train_test_splittrai">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2022-04-04T00:00:00.000Z">
<meta property="article:modified_time" content="2022-04-08T08:34:23.708Z">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="WB'blog" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">WB&#39;blog</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://woobinhwang.github.io"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-0404_Deep_Neural_Network" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2022/04/04/0404_Deep_Neural_Network/" class="article-date">
  <time class="dt-published" datetime="2022-04-04T00:00:00.000Z" itemprop="datePublished">2022-04-04</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      심층 신경망
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="심증-신경망"><a href="#심증-신경망" class="headerlink" title="심증 신경망"></a>심증 신경망</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br><span class="line"></span><br><span class="line">(train_input, train_target), (test_input, test_target) = keras.datasets.fashion_mnist.load_data()</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line">train_scaled = train_input / <span class="number">255.0</span>  <span class="comment"># 0 ~ 255의 픽셀값을 0~1로 변환</span></span><br><span class="line">train_scaled = train_scaled.reshape(-<span class="number">1</span>, <span class="number">28</span>*<span class="number">28</span>)  <span class="comment"># 2차원 배열을 1차원 배열로 변환</span></span><br><span class="line"></span><br><span class="line">train_scaled, val_scaled, train_target, val_target = train_test_split(</span><br><span class="line">    train_scaled, train_target, test_size=<span class="number">0.2</span>, random_state=<span class="number">42</span>)</span><br></pre></td></tr></table></figure>

<h1 id="심층-신경망-만들기"><a href="#심층-신경망-만들기" class="headerlink" title="심층 신경망 만들기"></a>심층 신경망 만들기</h1><ul>
<li>입력값 및 출력값 층 만들기</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">dense1 = keras.layers.Dense(<span class="number">100</span>, activation=<span class="string">&#x27;sigmoid&#x27;</span>, input_shape=(<span class="number">784</span>,))</span><br><span class="line">dense2 = keras.layers.Dense(<span class="number">10</span>, activation=<span class="string">&#x27;softmax&#x27;</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model = keras.Sequential([dense1, dense2])</span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure>

<pre><code>Model: &quot;sequential_7&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 dense_14 (Dense)            (None, 100)               78500     
                                                                 
 dense_15 (Dense)            (None, 10)                1010      
                                                                 
=================================================================
Total params: 79,510
Trainable params: 79,510
Non-trainable params: 0
_________________________________________________________________
</code></pre>
<h1 id="층을-추가하는-다른-방법"><a href="#층을-추가하는-다른-방법" class="headerlink" title="층을 추가하는 다른 방법"></a>층을 추가하는 다른 방법</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 직관적이게 한 줄로 적는 방법</span></span><br><span class="line">model = keras.Sequential([</span><br><span class="line">    keras.layers.Dense(<span class="number">100</span>, activation=<span class="string">&#x27;sigmoid&#x27;</span>, input_shape=(<span class="number">784</span>,), name=<span class="string">&#x27;hidden&#x27;</span>),</span><br><span class="line">    keras.layers.Dense(<span class="number">10</span>, activation=<span class="string">&#x27;softmax&#x27;</span>, name=<span class="string">&#x27;output&#x27;</span>)</span><br><span class="line">], name=<span class="string">&#x27;패션 MNIST 모델&#x27;</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.summary()</span><br></pre></td></tr></table></figure>

<pre><code>Model: &quot;패션 MNIST 모델&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 hidden (Dense)              (None, 100)               78500     
                                                                 
 output (Dense)              (None, 10)                1010      
                                                                 
=================================================================
Total params: 79,510
Trainable params: 79,510
Non-trainable params: 0
_________________________________________________________________
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># add()함수를 이용해 층을 추가하는 방법</span></span><br><span class="line">model = keras.Sequential()</span><br><span class="line">model.add(keras.layers.Dense(<span class="number">100</span>, activation=<span class="string">&#x27;sigmoid&#x27;</span>, input_shape=(<span class="number">784</span>,)))</span><br><span class="line">model.add(keras.layers.Dense(<span class="number">10</span>, activation=<span class="string">&#x27;softmax&#x27;</span>))</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.summary()</span><br></pre></td></tr></table></figure>

<pre><code>Model: &quot;sequential_8&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 dense_16 (Dense)            (None, 100)               78500     
                                                                 
 dense_17 (Dense)            (None, 10)                1010      
                                                                 
=================================================================
Total params: 79,510
Trainable params: 79,510
Non-trainable params: 0
_________________________________________________________________


- 은닉층의 layer 수: 785 * 100
- 결과층의 layer 수: (100 * 10) + 10 으로 추정됨.
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model.<span class="built_in">compile</span>(loss=<span class="string">&#x27;sparse_categorical_crossentropy&#x27;</span>, metrics=<span class="string">&#x27;accuracy&#x27;</span>)</span><br><span class="line"></span><br><span class="line">model.fit(train_scaled, train_target, epochs=<span class="number">5</span>)</span><br></pre></td></tr></table></figure>

<pre><code>Epoch 1/5
1500/1500 [==============================] - 4s 2ms/step - loss: 0.5608 - accuracy: 0.8098
Epoch 2/5
1500/1500 [==============================] - 3s 2ms/step - loss: 0.4056 - accuracy: 0.8525
Epoch 3/5
1500/1500 [==============================] - 3s 2ms/step - loss: 0.3716 - accuracy: 0.8661
Epoch 4/5
1500/1500 [==============================] - 3s 2ms/step - loss: 0.3491 - accuracy: 0.8735
Epoch 5/5
1500/1500 [==============================] - 4s 3ms/step - loss: 0.3319 - accuracy: 0.8800





&lt;keras.callbacks.History at 0x7f47a719add0&gt;
</code></pre>
<ul>
<li><p>층이 많은 심층 신경망일수록 학습을 어렵게 만들어 relu함수를 이용함.</p>
<ul>
<li>도출되는 확률값이 0이거나 음수이면 0을 출력.</li>
</ul>
</li>
<li><p>Flatten 클래스: 배치 차원을 제외하고 나머지 입력 차원을 모두 일렬로 펼치는 역할. Flatten층을 생성</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">model = keras.Sequential()</span><br><span class="line">model.add(keras.layers.Flatten(input_shape=(<span class="number">28</span>, <span class="number">28</span>)))</span><br><span class="line">model.add(keras.layers.Dense(<span class="number">100</span>, activation=<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line">model.add(keras.layers.Dense(<span class="number">10</span>, activation=<span class="string">&#x27;softmax&#x27;</span>))</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.summary()</span><br></pre></td></tr></table></figure>

<pre><code>Model: &quot;sequential_9&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 flatten_1 (Flatten)         (None, 784)               0         
                                                                 
 dense_18 (Dense)            (None, 100)               78500     
                                                                 
 dense_19 (Dense)            (None, 10)                1010      
                                                                 
=================================================================
Total params: 79,510
Trainable params: 79,510
Non-trainable params: 0
_________________________________________________________________
</code></pre>
<h1 id="옵티마이저"><a href="#옵티마이저" class="headerlink" title="옵티마이저"></a>옵티마이저</h1><ul>
<li><p>대체로 Adam을 사용하면 됨.</p>
</li>
<li><p>이유: 스텝 방향 &amp; 스템 사이즈를 모두 고려한 옵티마이저</p>
<ul>
<li>스텝 방향만 고려: GD, SGD, Momentum, NAG</li>
<li>스템 사이즈만 고려: GD, SGD, Adagrad, RMSProp</li>
</ul>
</li>
<li><p>오류 방지용 base</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">(train_input, train_target), (test_input, test_target) = keras.datasets.fashion_mnist.load_data()</span><br><span class="line"></span><br><span class="line">train_scaled = train_input / <span class="number">255.0</span></span><br><span class="line"></span><br><span class="line">train_scaled, val_scaled, train_target, val_target = train_test_split(</span><br><span class="line">    train_scaled, train_target, test_size=<span class="number">0.2</span>, random_state=<span class="number">42</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model.<span class="built_in">compile</span>(loss=<span class="string">&#x27;sparse_categorical_crossentropy&#x27;</span>, metrics=<span class="string">&#x27;accuracy&#x27;</span>)</span><br><span class="line">model.fit(train_scaled, train_target, epochs=<span class="number">5</span>)</span><br></pre></td></tr></table></figure>

<pre><code>Epoch 1/5
1500/1500 [==============================] - 5s 3ms/step - loss: 0.5277 - accuracy: 0.8145
Epoch 2/5
1500/1500 [==============================] - 4s 3ms/step - loss: 0.3914 - accuracy: 0.8606
Epoch 3/5
1500/1500 [==============================] - 7s 5ms/step - loss: 0.3551 - accuracy: 0.8724
Epoch 4/5
1500/1500 [==============================] - 5s 3ms/step - loss: 0.3351 - accuracy: 0.8807
Epoch 5/5
1500/1500 [==============================] - 3s 2ms/step - loss: 0.3198 - accuracy: 0.8857





&lt;keras.callbacks.History at 0x7f47a7af6e10&gt;
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.evaluate(val_scaled, val_target)</span><br></pre></td></tr></table></figure>

<pre><code>375/375 [==============================] - 1s 1ms/step - loss: 0.3423 - accuracy: 0.8812





[0.3422880470752716, 0.8811666369438171]
</code></pre>
<ul>
<li>옵티마이저 실습</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.<span class="built_in">compile</span>(optimizer=<span class="string">&#x27;sgd&#x27;</span>, loss=<span class="string">&#x27;sparse_categorical_crossentropy&#x27;</span>, metrics=<span class="string">&#x27;accuracy&#x27;</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sgd = keras.optimizers.SGD()</span><br><span class="line">model.<span class="built_in">compile</span>(optimizer=sgd, loss=<span class="string">&#x27;sparse_categorical_crossentropy&#x27;</span>, metrics=<span class="string">&#x27;accuracy&#x27;</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 학습률 지정</span></span><br><span class="line">sgd = keras.optimizers.SGD(learning_rate= <span class="number">0.1</span>)</span><br><span class="line"><span class="comment"># 모멘텀 최적화: 0보다 큰 값으로 지정하면 경사하강법을 가속도처럼 사용</span></span><br><span class="line">sgd = keras.optimizers.SGD(momentum= <span class="number">0.9</span>, nesterov= <span class="literal">True</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 적응적 학습률: 모델이 최적점에 가까이 갈수록 학습률을 낯추어, 안정적으로 최적점에 수렴 할 가능성이 높음.</span></span><br><span class="line"><span class="comment"># &#x27;adagrad&#x27; 를 이용</span></span><br><span class="line">adagrad =keras.optimizers.Adagrad()</span><br><span class="line">model.<span class="built_in">compile</span>(optimizer= adagrad, loss= <span class="string">&#x27;sparse_categoriacl_crossentropy&#x27;</span>,</span><br><span class="line">              metrics= <span class="string">&#x27;accuracy&#x27;</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 적응적 학습률</span></span><br><span class="line"><span class="comment"># &#x27;RMSprop&#x27; 을 이용</span></span><br><span class="line">rmsprop =keras.optimizers.RMSprop()</span><br><span class="line">model.<span class="built_in">compile</span>(optimizer= rmsprop, loss= <span class="string">&#x27;sparse_categoriacl_crossentropy&#x27;</span>,</span><br><span class="line">              metrics= <span class="string">&#x27;accuracy&#x27;</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 모멘텀 최적화와 RMSprop의 장점을 접목한것이 Adam</span></span><br><span class="line">model = keras.Sequential()</span><br><span class="line">model.add(keras.layers.Flatten(input_shape= (<span class="number">28</span>, <span class="number">28</span>)))</span><br><span class="line">model.add(keras.layers.Dense(<span class="number">100</span>, activation= <span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line">model.add(keras.layers.Dense(<span class="number">10</span>, activation= <span class="string">&#x27;softmax&#x27;</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model.<span class="built_in">compile</span>(optimizer= <span class="string">&#x27;adam&#x27;</span>, loss=<span class="string">&#x27;sparse_categorical_crossentropy&#x27;</span>, metrics=<span class="string">&#x27;accuracy&#x27;</span>)</span><br><span class="line">model.fit(train_scaled, train_target, epochs=<span class="number">5</span>)</span><br></pre></td></tr></table></figure>

<pre><code>Epoch 1/5
1500/1500 [==============================] - 3s 2ms/step - loss: 0.5213 - accuracy: 0.8182
Epoch 2/5
1500/1500 [==============================] - 3s 2ms/step - loss: 0.3931 - accuracy: 0.8588
Epoch 3/5
1500/1500 [==============================] - 3s 2ms/step - loss: 0.3477 - accuracy: 0.8730
Epoch 4/5
1500/1500 [==============================] - 3s 2ms/step - loss: 0.3233 - accuracy: 0.8807
Epoch 5/5
1500/1500 [==============================] - 3s 2ms/step - loss: 0.3053 - accuracy: 0.8876





&lt;keras.callbacks.History at 0x7f47a7af2a10&gt;
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.evaluate(val_scaled, val_target)</span><br></pre></td></tr></table></figure>

<pre><code>375/375 [==============================] - 1s 1ms/step - loss: 0.3386 - accuracy: 0.8772





[0.33857372403144836, 0.8771666884422302]
</code></pre>
<h1 id="마무리-정리"><a href="#마무리-정리" class="headerlink" title="마무리 정리"></a>마무리 정리</h1><ul>
<li>키워드<ul>
<li><strong>심층 신경망</strong>: 2개 이상의 층을 포함한 신경망. 종종 다층 인공 신경망, 심층 신경망, 딥러닝을 같은 의미로 사용</li>
<li><strong>은닉층</strong>: 입력층과 출력층 사이에 있는 모든 층</li>
<li><strong>렐루 함수</strong>: 이미지 분류 모델의 은닉층을 많이 사용하는 활성화 함수</li>
<li><strong>옵티마이저</strong>: 신경망의 가중치와 절편을 학습하기 위한 알고리즘 또는 방법</li>
</ul>
</li>
<li><strong>TensorFlow 패키지</strong><ul>
<li><strong>add()</strong>: 케라스 모델에 층을 추가하는 메서드</li>
<li><strong>summary()</strong>: 케라스 모델의 정보를 출력</li>
<li><strong>SGD</strong>: 기본 경사 하강법 옵티마이저 클래스</li>
<li><strong>Adagrad</strong>: Adagrad 옵티마이저 클래스</li>
<li><strong>RMSprop</strong>: RMSprop옵티마이저 클래스</li>
<li><strong>Adam</strong>: Adam옵티마이저 클래스</li>
</ul>
</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://woobinhwang.github.io/2022/04/04/0404_Deep_Neural_Network/" data-id="cl1km1rbd0001pwnh9ypc2bp1" data-title="심층 신경망" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2022/04/04/0404_Artificial_Neural_Network/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          인공 신경망
        
      </div>
    </a>
  
  
    <a href="/2022/03/31/0331_Principar_Component_Analysis_01/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">주성분_분석_01</div>
    </a>
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/04/">April 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/03/">March 2022</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2022/04/08/0408_LSTM,GRU_Cell/">LSTM과 GRU셀</a>
          </li>
        
          <li>
            <a href="/2022/04/08/0408_IMDB/">IMDB리뷰</a>
          </li>
        
          <li>
            <a href="/2022/04/07/0407_Sequentoal_Data,Recurrent_Neural_Network/">순차 데이터와 순환 신경망</a>
          </li>
        
          <li>
            <a href="/2022/04/07/0407_Convolution_Neural_Network_02/">합성곱 신경망_02</a>
          </li>
        
          <li>
            <a href="/2022/04/06/0406_Convolution_Neural_Network/">합성곱 신경망_01</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2022 John Doe<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.4.1.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>